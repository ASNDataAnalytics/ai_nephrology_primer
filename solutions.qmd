---
title: "Solutions"
format: html
code-fold: true
code-copy: true
---

::: callout-important
Note that LLMs are stochastic/non-deteriministic, so there are no definitive solutions.
:::

## Possible Solutions to Exercises

### Exercise 1

**Prompt:** Use an LLM to clean data in preparation for analysis and modeling.

**Possible Solutions**

::: {.panel-tabset group="llm1"}
## ChatGPT

**Prompt:** You are Dr. Karandeep Singh, the creator of TidierOrg and Tidier.jl. You are an expert in coding, AI, and nephrology.

\[Objective\] Clean messy clinical trial data to prepare for analysis and predictive modeling.

\[Context\] Take this messy data {messy_aki.xlsx} and provide valid Julia code to accurately clean the data.

\[Constraints\] You must use TidierOrg tools to clean the data.

\[Format\] Return valid Julia code to read in the messy data, clean and validate the data, and export the data to a csv. Provide a report in markdown that catalogs missing data, outliers, and any edge cases that need HITL review before modeling. Clearly explain which Tidier.jl packages you are using and what each section of the code achieves with the resulting Julia code you generate. Do this in a manner for someone who is familiar with code but knows nothing about Julia. Provide key points for choosing Julia over other programming languages.

**Response:**

Julia cleaning script (TidierOrg-first)

```{julia}

############################################################
# AKI trial data cleaning (TidierOrg / Tidier.jl workflow)
# - Reads messy Excel
# - Detects the true header row
# - Cleans/standardizes fields (dates, categories, numerics)
# - Validates + flags HITL edge cases
# - Exports clean CSV + a markdown QA report
############################################################

using Tidier                    # meta-package (re-exports TidierData, TidierFiles, TidierStrings, TidierDates, etc.)
using DataFrames
using Dates
using Statistics

# -------------------------
# 0) I/O paths
# -------------------------
in_path  = "/mnt/data/messy_aki.xlsx"
out_csv  = "aki_clean.csv"
out_md   = "aki_cleaning_report.md"

# -------------------------
# 1) Read Excel (TidierFiles)
#    We read with col_names=false because the file has title rows above the real header
# -------------------------
df_raw = TidierFiles.read_xlsx(in_path; sheet=1, col_names=false, trim_ws=true)

# -------------------------
# 2) Find the header row containing "study_id" in the first column, then re-read as data
# -------------------------
firstcol = df_raw[:, 1]
header_row = findfirst(x -> !ismissing(x) && lowercase(String(x)) == "study_id", firstcol)

if header_row === nothing
    error("Could not find a header row with 'study_id' in column 1. Inspect df_raw to locate headers.")
end

# Extract header values and data rows
hdr_vals = df_raw[header_row, :]
df = df_raw[(header_row + 1):nrow(df_raw), :]

# Assign column names from the header row
rename!(df, Symbol.(map(x -> ismissing(x) ? "" : String(x), collect(hdr_vals))))

# Drop any columns with empty names (common when there are blank columns in the sheet)
empty_name_cols = names(df)[map(nm -> nm == Symbol(""), names(df))]
if !isempty(empty_name_cols)
    select!(df, Not(empty_name_cols))
end

# Clean column names to snake_case (TidierData)
df = @chain df begin
    @clean_names()
end

# -------------------------
# 3) Drop trailing note rows / non-data rows
#    (in this file, rows with missing study_id are notes/permissions)
# -------------------------
df = @chain df begin
    @filter(!ismissing(study_id))
end

# -------------------------
# 4) Helper parsers (used inside @mutate)
# -------------------------
# Extract the first number from messy strings like "19 but poor collection"
function first_number(x)
    if ismissing(x)
        return missing
    end
    if x isa Number
        return Float64(x)
    end
    s = strip(String(x))
    m = match(r"-?\d+(\.\d+)?", s)
    return m === nothing ? missing : parse(Float64, m.match)
end

# Parse DOB from multiple common Excel/string formats
function parse_dob(x)
    if ismissing(x)
        return missing
    end
    # Already a Date/DateTime
    if x isa Date
        return x
    elseif x isa DateTime
        return Date(x)
    end

    s = strip(String(x))
    if isempty(s)
        return missing
    end

    # Normalize month-name strings like "may 5 1970" -> "May 5 1970"
    s2 = join(uppercasefirst.(split(lowercase(s), ' ')), ' ')

    # Try a few formats
    for fmt in (dateformat"y-m-d", dateformat"m/d/y", dateformat"u d y", dateformat"u d, y")
        try
            return Date(s2, fmt)
        catch
        end
    end

    # Last resort: try Date(s2) (can work for some ISO-ish strings)
    try
        return Date(s2)
    catch
        return missing
    end
end

# Standardize Yes/No-ish fields
function yesno(x)
    if ismissing(x)
        return missing
    end
    s = lowercase(strip(String(x)))
    if s in ("yes", "y", "true", "1")
        return "Yes"
    elseif s in ("no", "n", "false", "0")
        return "No"
    else
        return missing
    end
end

# Standardize sex
function sex_std(x)
    if ismissing(x)
        return missing
    end
    s = lowercase(strip(String(x)))
    if s in ("m", "male")
        return "Male"
    elseif s in ("f", "female")
        return "Female"
    else
        return missing
    end
end

# Standardize race (example mapping—edit if your protocol uses different groupings)
function race_std(x)
    if ismissing(x)
        return missing
    end
    s = lowercase(strip(String(x)))
    if s in ("white", "caucasian", "w", "white ")
        return "White"
    elseif s in ("black", "af-am", "african american", "african-american")
        return "Black"
    elseif s in ("asian")
        return "Asian"
    elseif s in ("mixed", "multiracial")
        return "Mixed"
    elseif s in ("other")
        return "Other"
    else
        return "Other"
    end
end

# Standardize Hispanic ethnicity with typo handling
function hispanic_std(x)
    if ismissing(x)
        return missing
    end
    s = lowercase(strip(String(x)))
    if s in ("hispanic", "h", "h ", "hisspanic", "hispannic")
        return "Hispanic"
    elseif s in ("not hispanic", "nh", "non-hispanic", "non hispanic")
        return "Not Hispanic"
    else
        return missing
    end
end

# -------------------------
# 5) Main cleaning transforms (TidierData)
#    - Parse types
#    - Keep “raw” copies where useful for audit
#    - Create QA flags for HITL review
# -------------------------
df_clean = @chain df begin
    @mutate(
        # IDs
        study_id = as_integer(study_id),

        # DOB -> Date
        dob_raw = dob,
        dob = ~parse_dob(dob),

        # Demographics standardization
        sex_raw = sex,
        sex = ~sex_std(sex),

        race_raw = race,
        race = ~race_std(race),

        hispanic_raw = hispanic,
        hispanic = ~hispanic_std(hispanic),

        # GFR numerics (keep raw columns + parsed numeric columns)
        admission_gfr_raw = admission_gfr,
        day_1_gfr_raw      = day_1_gfr,
        day2gfr_raw        = day2gfr,
        day_3_gfr_raw      = day_3_gfr,
        day_4_gfr_raw      = day_4_gfr,
        day5_gfr_raw       = day5_gfr,

        admission_gfr = ~first_number(admission_gfr),
        day_1_gfr     = ~first_number(day_1_gfr),
        day2_gfr      = ~first_number(day2gfr),
        day_3_gfr     = ~first_number(day_3_gfr),
        day_4_gfr     = ~first_number(day_4_gfr),
        day5_gfr      = ~first_number(day5_gfr),

        # Flags: “had extra text” (e.g., "19 but poor collection")
        day2_parse_note = !(ismissing(day2gfr_raw)) && !(String(day2gfr_raw) |> strip |> x -> occursin(r"^-?\d+(\.\d+)?$", x)),

        # Devices
        hd_catheter_raw = hd_catheter,
        hd_catheter = ~yesno(hd_catheter),

        av_fistula_raw = av_fistula,
        av_fistula = ~yesno(av_fistula)
    ) |>

    # Keep a tidy column order (optional)
    @select(
        study_id, dob, sex, race, hispanic,
        admission_gfr, day_1_gfr, day2_gfr, day_3_gfr, day_4_gfr, day5_gfr,
        diagnosis, sediment, hd_catheter, av_fistula,
        day2_parse_note,
        # audit columns at the end
        dob_raw, sex_raw, race_raw, hispanic_raw,
        admission_gfr_raw, day_1_gfr_raw, day2gfr_raw, day_3_gfr_raw, day_4_gfr_raw, day5_gfr_raw,
        hd_catheter_raw, av_fistula_raw
    )
end

# -------------------------
# 6) Validation + HITL flags
# -------------------------

# Duplicated study_id check
dup_ids = @chain df_clean begin
    @count(study_id)
    @filter(n > 1)
end

# Missingness summary
missing_summary = DataFrame(
    column = String[],
    n_missing = Int[],
    pct_missing = Float64[]
)
for nm in names(df_clean)
    col = df_clean[!, nm]
    nmiss = count(ismissing, col)
    push!(missing_summary, (String(nm), nmiss, round(100 * nmiss / nrow(df_clean); digits=1)))
end

# Outlier thresholds (IQR rule) for each GFR column
gfr_cols = [:admission_gfr, :day_1_gfr, :day2_gfr, :day_3_gfr, :day_4_gfr, :day5_gfr]

function iqr_bounds(v::Vector{Union{Missing, Float64}})
    x = collect(skipmissing(v))
    if length(x) < 4
        return (missing, missing)  # too few points
    end
    q1 = quantile(x, 0.25)
    q3 = quantile(x, 0.75)
    iqr = q3 - q1
    return (q1 - 1.5*iqr, q3 + 1.5*iqr)
end

outlier_rows = DataFrame()
for c in gfr_cols
    lo, hi = iqr_bounds(df_clean[!, c])
    if !(ismissing(lo) || ismissing(hi))
        tmp = @chain df_clean begin
            @mutate(_col = String(c), _lo = lo, _hi = hi)
            @filter(!ismissing($(c)) && (($(c) < lo) || ($(c) > hi)))
            @select(study_id, _col, $(c), _lo, _hi)
        end
        outlier_rows = vcat(outlier_rows, tmp; cols=:union)
    end
end

# Extra clinical sanity flags (edit thresholds to your protocol)
sanity_flags = @chain df_clean begin
    @mutate(
        flag_dob_unparsed = ismissing(dob),
        flag_gfr_negative = any.((admission_gfr, day_1_gfr, day2_gfr, day_3_gfr, day_4_gfr, day5_gfr) .< 0),
        flag_gfr_very_high = any.((admission_gfr, day_1_gfr, day2_gfr, day_3_gfr, day_4_gfr, day5_gfr) .> 200),
        flag_day3_missing = ismissing(day_3_gfr),
        flag_day2_note = day2_parse_note
    )
end

# -------------------------
# 7) Export clean CSV (TidierFiles)
# -------------------------
TidierFiles.write_csv(df_clean, out_csv)

# -------------------------
# 8) Write a markdown QA report
# -------------------------
function df_to_md_table(d::DataFrame; max_rows::Int=50)
    n = min(nrow(d), max_rows)
    cols = names(d)
    header = "|" * join(string.(cols), "|") * "|\n"
    sep    = "|" * join(fill("---", length(cols)), "|") * "|\n"
    rows = ""
    for i in 1:n
        vals = [ismissing(d[i, c]) ? "missing" : string(d[i, c]) for c in cols]
        rows *= "|" * join(vals, "|") * "|\n"
    end
    if nrow(d) > max_rows
        rows *= "\n*(truncated to first $(max_rows) rows)*\n"
    end
    return header * sep * rows
end

report = """
# AKI Data Cleaning QA Report

## Files
- Input: `$(in_path)`
- Output CSV: `$(out_csv)`

## Row counts
- Rows after header detection + dropping note rows: **$(nrow(df_clean))**
- Columns: **$(ncol(df_clean))**

## Missing data (all columns)
$(df_to_md_table(sort(missing_summary, :n_missing, rev=true)))

## Duplicate study IDs (requires HITL)
$(nrow(dup_ids) == 0 ? "_None detected._" : df_to_md_table(dup_ids))

## Parsed-text notes (e.g., GFR cell had extra text)
Rows with `day2_parse_note = true` should be reviewed to confirm the numeric extraction matches intent.
$(df_to_md_table(@chain df_clean begin
    @filter(day2_parse_note == true)
    @select(study_id, day2gfr_raw, day2_gfr)
end))

## Outliers by IQR rule (requires HITL)
$(nrow(outlier_rows) == 0 ? "_None detected by IQR rule._" : df_to_md_table(outlier_rows))

## Sanity flags (requires HITL)
$(df_to_md_table(@chain sanity_flags begin
    @filter(flag_dob_unparsed || flag_gfr_very_high || flag_day3_missing || flag_day2_note)
    @select(study_id, flag_dob_unparsed, flag_gfr_very_high, flag_day3_missing, flag_day2_note,
            admission_gfr, day_1_gfr, day2_gfr, day_3_gfr, day_4_gfr, day5_gfr,
            dob_raw)
end))
"""

open(out_md, "w") do io
    write(io, report)
end

println("Wrote clean CSV -> $(out_csv)")
println("Wrote QA report -> $(out_md)")

```

**What TidierOrg packages this uses (and why)**

-   **Tidier.jl (meta-package)**: loads the “tidyverse-like” ecosystem in one import. Tidier.jl is designed as a meta-package that re-exports functionality from smaller TidierOrg packages.

-   **TidierFiles.jl**: provides `read_xlsx()` and `write_csv()` with a consistent “readr/readxl-ish” API, powered by XLSX.jl/CSV.jl under the hood.

-   **TidierData.jl**: provides tidy macros like `@chain`, `@mutate`, `@filter`, `@select`, and `@clean_names()` for dplyr/tidyr-like transformations.

Markdown QA report (based on *this* `messy_aki.xlsx`)

**Summary**

-   **Excel rows:** 27 total on the sheet

-   **True data table rows:** 23 (after skipping title/header junk)

-   **Non-data note rows removed:** 3 (permission/notes at bottom with missing `study_id`)

-   **Final analytic rows:** **20**

-   **Key HITL items:** duplicate study IDs, extreme/implausible GFR values, one missing Day 3 GFR, “numeric-with-text” cell.

**Missing data**

-   `day_3_gfr`: **1 missing** (study_id **21**)

**Outliers (needs HITL review)**

Using an IQR rule on the numeric-parsed GFR values, these stand out:

-   **Admission GFR = 232** (study_id **20**) — likely a data entry error (e.g., “23.2” or “23”?) given subsequent days are \~19–33.

-   **Admission GFR = 177** (study_id **21**) — unusually high vs cohort; may be real or a typo (“17.7” / “17”?).

-   **Day 1 GFR = 322** (study_id **15**) — almost certainly a typo (pattern suggests “32”).

**Edge cases / messy-field normalization**

-   **DOB** includes a free-text date (`"may 5 1970"`) that must be parsed carefully (the script handles multiple formats).

-   **Sex** mixes `Male/Female` with `M/F` and trailing whitespace.

-   **Race** mixes casing and abbreviations (`WHITE`, `Af-Am`, `Caucasian`, etc.).

-   **Hispanic** includes typos (`Hisspanic`, `Hispannic`, `H`) and shorthand (`NH`).

-   **Day2 GFR** contains at least one cell like `"19 but poor collection"`; the script extracts the first numeric value (=19) **and flags it** (`day2_parse_note=true`) for review.

**ID integrity (must resolve before modeling)**

-   **Duplicate `study_id`: 3 appears twice; 8 appears twice.**\
    These represent clearly different patients/records and should be reconciled (true duplicates vs mis-keyed IDs) before train/test splits.

**How to read the Julia code if you “know code but not Julia”**

-   `using Tidier` is like `library(tidyverse)`—it brings in the TidierOrg ecosystem.

-   `@chain df begin ... end` is like `%>%` in R or method-chaining in Python; it pipes a DataFrame through steps.

-   `@mutate(new = f(old))` creates/overwrites columns.

-   `missing` is Julia’s NA.

-   The `~` prefix inside TidierData macros tells TidierData **not** to auto-vectorize a function (important when you’re calling helper functions that should run “per-cell” rather than being broadcast in a surprising way).

**Why choose Julia here (vs Python/R) — the practical version**

-   **Speed without giving up expressiveness**: you can do heavy feature engineering and modeling in one language without “two-language” rewrites.

-   **Composable, high-performance tabular + ML stack**: DataFrames + MLJ/Flux/etc. can stay fast as the pipeline grows.

-   **Tidyverse-like ergonomics (TidierOrg)**: if your team thinks in `dplyr` verbs, TidierData lowers the adoption cost while staying Julia-native.

If you want, I can adapt the race/ethnicity mappings and the GFR plausibility thresholds to match your trial’s data dictionary / SAP (e.g., “Unknown” handling, allowable ranges, and whether GFR is eGFR vs measured clearance).

## Claude

**Prompt:** can you show me some python code to clean this data up?

**Response:**

Here's a well-commented cleaning script that tackles every issue I spotted:

```{python}
#| eval: false         
import pandas as pd
import numpy as np
from dateutil import parser as dateparser
import re

# ── 1. Load, skip the 3-row title block, drop the empty column F ──────────────
df = pd.read_excel("messy_aki.xlsx", skiprows=4, header=0)
df = df.drop(columns=[df.columns[5]])          # unnamed empty column between Hispanic and Admission GFR
df = df.dropna(how="all")                       # drop fully-empty rows (incl. footer notes)

# ── 2. Standardise column names ───────────────────────────────────────────────
df.columns = [
    "study_id", "dob", "race", "sex", "hispanic",
    "gfr_admission", "gfr_day1", "gfr_day2", "gfr_day3", "gfr_day4", "gfr_day5",
    "diagnosis", "sediment", "hd_catheter", "av_fistula"
]

# ── 3. Duplicate / invalid study IDs ─────────────────────────────────────────
# IDs 3 and 8 appear twice; IDs 9, 10, 18 are missing entirely.
# Flag duplicates so you can review — don't silently drop them.
df["duplicate_id_flag"] = df.duplicated(subset="study_id", keep=False)

# ── 4. Parse dates of birth ───────────────────────────────────────────────────
def parse_dob(val):
    if pd.isna(val):
        return pd.NaT
    if isinstance(val, pd.Timestamp):
        return val
    # Handle obvious typo like "5=11-1984"
    val = str(val).replace("=", "-")
    try:
        return dateparser.parse(val, dayfirst=False)
    except Exception:
        return pd.NaT

df["dob"] = pd.to_datetime(df["dob"].apply(parse_dob), errors="coerce")

# ── 5. Standardise race ───────────────────────────────────────────────────────
race_map = {
    "caucasian": "White",
    "white":     "White",
    "af-am":     "Black",
    "black":     "Black",
    "asian":     "Asian",
    "mixed":     "Other",
    "other":     "Other",
}
df["race"] = (df["race"]
              .str.strip()
              .str.lower()
              .map(race_map)
              .fillna("Unknown"))

# ── 6. Standardise sex ────────────────────────────────────────────────────────
def clean_sex(val):
    if pd.isna(val):
        return np.nan
    v = str(val).strip().lower()
    if v in ("male", "m"):
        return "Male"
    if v in ("female", "f"):
        return "Female"
    return np.nan

df["sex"] = df["sex"].apply(clean_sex)

# ── 7. Standardise hispanic ethnicity ────────────────────────────────────────
def clean_hispanic(val):
    if pd.isna(val):
        return np.nan
    v = str(val).strip().lower().replace(" ", "")
    if v in ("hispanic", "h", "hispannic", "hisspanic"):   # typos
        return "Hispanic"
    if v in ("nothispanic", "nh"):
        return "Not Hispanic"
    return np.nan

df["hispanic"] = df["hispanic"].apply(clean_hispanic)

# ── 8. Clean GFR columns — coerce text to numeric, flag outliers ──────────────
gfr_cols = ["gfr_admission", "gfr_day1", "gfr_day2", "gfr_day3", "gfr_day4", "gfr_day5"]

def extract_numeric(val):
    """Pull the first number out of a cell; return NaN if none found."""
    if pd.isna(val):
        return np.nan
    try:
        return float(val)
    except (ValueError, TypeError):
        match = re.search(r"\d+\.?\d*", str(val))
        return float(match.group()) if match else np.nan

for col in gfr_cols:
    df[col] = df[col].apply(extract_numeric)

# Flag physiologically implausible GFR values (> 200 are likely data-entry errors)
GFR_MAX = 200
for col in gfr_cols:
    flag_col = col + "_outlier_flag"
    df[flag_col] = df[col] > GFR_MAX

# ── 9. Standardise Yes/No columns ────────────────────────────────────────────
def clean_yesno(val):
    if pd.isna(val):
        return np.nan
    v = str(val).strip().lower().replace("`", "")   # "No`" typo
    if v == "yes":
        return "Yes"
    if v == "no":
        return "No"
    return np.nan

for col in ["hd_catheter", "av_fistula"]:
    df[col] = df[col].apply(clean_yesno)

# ── 10. Standardise sediment free-text (minor normalisation) ─────────────────
df["sediment"] = (df["sediment"]
                  .str.strip()
                  .str.lower()
                  .str.replace(r"\s+", " ", regex=True)
                  .str.replace("epi cell", "epithelial cell")
                  .str.replace("gran ", "granular "))

# ── 11. Report a cleaning summary ─────────────────────────────────────────────
print("=== Cleaning summary ===")
print(f"Rows: {len(df)}")
print(f"\nDuplicate study IDs:\n{df[df['duplicate_id_flag']][['study_id','dob','diagnosis']]}")
print(f"\nUnparseable DOBs: {df['dob'].isna().sum()}")
print(f"\nGFR outlier flags:")
for col in gfr_cols:
    n = df[col + "_outlier_flag"].sum()
    if n:
        print(f"  {col}: {n} outlier(s) — {df.loc[df[col+'_outlier_flag'], ['study_id', col]].values.tolist()}")

print(f"\nSex nulls after cleaning:      {df['sex'].isna().sum()}")
print(f"Hispanic nulls after cleaning: {df['hispanic'].isna().sum()}")
print(f"Race unknowns:                 {(df['race']=='Unknown').sum()}")

# ── 12. Save ──────────────────────────────────────────────────────────────────
df.to_csv("aki_clean.csv", index=False)
print("\nSaved to aki_clean.csv")

```

```         
=== Cleaning summary ===
Rows: 22

Duplicate study IDs:
    study_id        dob           diagnosis
2        3.0 1970-05-05        bleeding out
3        3.0 1981-06-03  norovirus diarrhea
8        8.0 1982-04-22     C diff diarrhea
9        8.0 2003-10-12           NSAID tox
21       NaN        NaT                 NaN
22       NaN        NaT                 NaN

Unparseable DOBs: 2

GFR outlier flags:
  gfr_admission: 1 outlier(s) — [[20.0, 232.0]]
  gfr_day1: 1 outlier(s) — [[15.0, 322.0]]

Sex nulls after cleaning:      2
Hispanic nulls after cleaning: 2
Race unknowns:                 2

Saved to aki_clean.csv
```

Here's a rundown of every issue it handles:

**Structure** — skips the 3-row title block ("Acute Kidney Injury Study", resident name, project name) and the stray empty column between Hispanic and Admission GFR.

**Column names** — normalises the wildly inconsistent headers (``` DAY2GFR\``, ```day 3 GfR\`, etc.) to snake_case.

**Duplicate IDs** — study_ids 3 and 8 each appear twice (and 9, 10, 18 are missing). The script flags duplicates rather than silently dropping them, since you'll need to decide which row is correct.

**Dates of birth** — handles three formats: Excel dates, "may 5 1970" (free text), and "5=11-1984" (typo with `=` instead of `-`).

**Race** — merges "Caucasian"/"WHITE"/"white" → "White", "Af-Am"/"Black" → "Black", etc.

**Sex** — normalises "M ", "F ", "Male", "Female", "F" → consistent "Male"/"Female".

**Hispanic** — fixes typos ("Hispannic", "Hisspanic") and abbreviations ("H", "NH").

**GFR values** — row 2 has "19 but poor collection" (extracts 19), and several rows have values like 322 or 232 that are almost certainly data-entry errors (extra digit). Those are flagged with outlier columns.

**Yes/No fields** — handles "YES", "NO", "No\`" (backtick typo) → "Yes"/"No".

## Gemini

**Prompt:** can you show me some python code to clean this data up?

**Response:**

```{r}
#| eval: false

```
:::

### Exercise 2

**Prompt:** Use an LLM to clean data in preparation for analysis and modeling.

**Possible Solutions**

::: {.panel-tabset group="llm2"}
## ChatGPT

**Prompt:** can you show me some python code to clean this data up?\
\
**Response:**

```{julia}
#| eval: false

```

## Claude

**Prompt:** you are an expert python coder. Using polars and scikitlearn, write code to predict remission status (1=remission) using a dataset with the following variables:

Variables

days_of_life - age in days. Numeric. Range: 1207-32356. 1 missing value. plt - Platelet Count. Numeric. Range: 11-1114. 4 missing values. mpv - Mean Platelet Volume. Numeric. Range: 5.3-13.5. 21 missing values. un - Blood Urea Nitrogen. Numeric. Range: 2-118. 53 missing values. wbc - White Blood Cell Count. Numeric. Range: 0.7-33.5. No missing values. hgb - Hemoglobin. Numeric. Range: 4.5-18.6. 4 missing values. hct Hematocrit. Numeric. Range: 13.7-55.2. 3 missing values. rbc - Red Blood Cell Count. Numeric. Range: 1.57-7.04. 3 missing values. mcv - Mean Corpuscular (RBC) Volume. Numeric. Range: 56.5-124. 3 missing values. mch - Mean Corpuscular (RBC) Hemoglobin. Numeric. Range: 16.7-42.3. 7 missing values. mchc - Mean Corpuscular (RBC) Hemoglobin per Cell. Numeric. Range: 28.2-38.0. 7 missing values. rdw - Red cell Distribution Width. Numeric. Range: 11.3-39.7. 3 missing values. neut_percent - Percent of Neutrophils in WBC count. Numeric. Range: 17-98.1. No missing values. lymph_percent - Percent of Lymphocytes in WBC count. Numeric. Range: 1-67.9. No missing values. mono_percent - Percent of Monocytes in WBC count. Numeric. Range: 0-30.3. No missing values. eos_percent - Percent of Eosinophils in WBC count. Numeric. Range: 0.5-29.3. 6 missing values. baso_percent - Percent of Basoophils in WBC count. Numeric. Range: 0.2-5.3. 6 missing values. sod - Sodium. Numeric. Range: 116-151. No missing values. pot - Potassium. Numeric. Range: 2.6-10.1. 1 missing value. chlor - Chloride. Numeric. Range: 83-126. No missing values. co2 - Bicarbonate (CO2). Numeric. Range: 12-40. 5 missing values. creat - Creatinine. Numeric. Range: 0.2-8.4. No missing values. gluc - Glucose. Numeric. Range: 41-486. No missing values. cal - Calcium. Numeric. Range: 6.5-11.8. 1 missing value. prot - Protein. Numeric, range 2.9-10, 0 missing values alb - Albumin. Numeric, range 1.2-5.5, 0 missing values ast - Aspartate Transaminase. Numeric, range 5-7765, 0 missing values alt - Alanine Transaminase. Numeric, range 1-10666, 18 missing values alk - Alkaline phosphatase. Numeric, range 13-1938, 0 missing values tbil - Total Bilirubin. Numeric, range 0.09-27, 0 missing values active - Active Inflammation despite Thiopurines for \> 12 weeks. Numeric, range 0-1, 0 missing values remission - Remission of Inflammation after Thiopurines for \> 12 weeks. Numeric, range 0-1, 0 missing values

**Response:**

Here's a complete script using Polars and scikit-learn to predict remission status:

```{python}
#| eval: false
import polars as pl
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold, cross_validate
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    classification_report, roc_auc_score, average_precision_score,
    make_scorer, f1_score
)
import warnings
warnings.filterwarnings("ignore")

# ── 1. Load data ──────────────────────────────────────────────────────────────
df = pl.read_csv("your_data.csv")          # ← adjust path / format as needed

# ── 2. Define features & target ───────────────────────────────────────────────
TARGET = "remission"

FEATURES = [
    "days_of_life", "plt", "mpv", "un", "wbc", "hgb", "hct", "rbc",
    "mcv", "mch", "mchc", "rdw",
    "neut_percent", "lymph_percent", "mono_percent", "eos_percent", "baso_percent",
    "sod", "pot", "chlor", "co2", "creat", "gluc", "cal",
    "prot", "alb", "ast", "alt", "alk", "tbil",
    "active",        # clinical flag — strong prior candidate
]

# ── 3. Polars preprocessing ───────────────────────────────────────────────────
# Log-transform right-skewed lab values to reduce outlier influence
skewed = ["ast", "alt", "alk", "tbil", "gluc", "wbc", "plt", "un", "creat"]

df = df.with_columns([
    pl.col(c).log1p().alias(c) for c in skewed if c in df.columns
])

# Extract numpy arrays (Polars → numpy for sklearn)
X = df.select(FEATURES).to_numpy().astype(np.float64)
y = df[TARGET].to_numpy().astype(int)

print(f"Dataset shape : {X.shape}")
print(f"Class balance : {np.bincount(y)} (0=no remission, 1=remission)")

# ── 4. Build pipelines ────────────────────────────────────────────────────────
# Median imputation handles the missing values described above; scaler for LR
imputer  = SimpleImputer(strategy="median")
scaler   = StandardScaler()

pipelines = {
    "Logistic Regression": Pipeline([
        ("impute", imputer),
        ("scale",  scaler),
        ("clf",    LogisticRegression(max_iter=1000, class_weight="balanced",
                                      C=0.1, solver="lbfgs")),
    ]),
    "Random Forest": Pipeline([
        ("impute", imputer),
        ("clf",    RandomForestClassifier(
            n_estimators=500, max_features="sqrt",
            class_weight="balanced", random_state=42, n_jobs=-1
        )),
    ]),
    "Gradient Boosting": Pipeline([
        ("impute", imputer),
        ("clf",    GradientBoostingClassifier(
            n_estimators=300, learning_rate=0.05,
            max_depth=3, subsample=0.8, random_state=42
        )),
    ]),
}

# ── 5. Cross-validation ───────────────────────────────────────────────────────
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

scorers = {
    "roc_auc"  : "roc_auc",
    "avg_prec" : "average_precision",
    "f1"       : make_scorer(f1_score, zero_division=0),
    "accuracy" : "accuracy",
}

results = {}
for name, pipe in pipelines.items():
    cv_res = cross_validate(pipe, X, y, cv=cv, scoring=scorers,
                            return_train_score=False)
    results[name] = {k: cv_res[f"test_{k}"] for k in scorers}
    print(f"\n{'─'*50}")
    print(f"  {name}")
    print(f"{'─'*50}")
    for metric, vals in results[name].items():
        print(f"  {metric:<12}  {vals.mean():.3f}  ±  {vals.std():.3f}")

# ── 6. Feature importance (Random Forest) ────────────────────────────────────
print("\n\n── Feature Importance (Random Forest) ──")
rf_pipe = pipelines["Random Forest"]
rf_pipe.fit(X, y)
importances = rf_pipe.named_steps["clf"].feature_importances_

feat_df = (
    pl.DataFrame({"feature": FEATURES, "importance": importances})
    .sort("importance", descending=True)
)
print(feat_df)

# ── 7. Final model + hold-out evaluation (optional) ──────────────────────────
# Uncomment if you have a separate test set:

# df_test = pl.read_csv("test_data.csv")
# X_test  = df_test.select(FEATURES).to_numpy().astype(np.float64)
# y_test  = df_test[TARGET].to_numpy().astype(int)
#
# best_pipe = pipelines["Random Forest"]   # swap to whichever won CV
# best_pipe.fit(X, y)
# y_pred  = best_pipe.predict(X_test)
# y_prob  = best_pipe.predict_proba(X_test)[:, 1]
#
# print(classification_report(y_test, y_pred, target_names=["No Remission","Remission"]))
# print(f"ROC-AUC : {roc_auc_score(y_test, y_prob):.3f}")
# print(f"Avg Prec: {average_precision_score(y_test, y_prob):.3f}")
```

**Key design decisions explained:**

**Missing data** — `SimpleImputer(strategy="median")` handles all the missingness you described. Median is preferred over mean here because several lab values (AST, ALT, ALK, BUN) are heavily right-skewed.

**Log transforms** — Applied via Polars before the sklearn pipeline for AST, ALT, ALK, tbil, glucose, WBC, platelets, BUN, and creatinine. These are notorious for extreme outliers in clinical labs that can destabilize linear models and impair tree splits.

**Class imbalance** — `class_weight="balanced"` is set for both Logistic Regression and Random Forest, which automatically up-weights the minority class. GBM handles this implicitly through its loss function but you can add `scale_pos_weight` if imbalance is severe.

**Three model types** — Logistic Regression gives interpretable coefficients and a calibration baseline; Random Forest is robust to multicollinearity (which will be present among the CBC components like HGB/HCT/RBC); Gradient Boosting often wins on tabular clinical data.

**Evaluation metrics** — ROC-AUC, Average Precision (PR-AUC), and F1 are all reported because accuracy alone is misleading with class imbalance.

**`active` feature** — Included intentionally since active inflammation is clinically the most direct predictor of remission failure; if you want a "lab-only" model, simply remove it from `FEATURES`.

## Gemini

**Prompt:** can you show me some python code to clean this data up?

**Response:**

```{r}
#| eval: false

```
:::

### Exercise 3

**Prompt:** Use an LLM to clean data in preparation for analysis and modeling.

**Possible Solutions**

::: {.panel-tabset group="llm3"}
## ChatGPT

**Prompt:** can you show me some python code to clean this data up?

**Response:**

```{julia}
#| eval: false


```

## Claude

**Prompt:** can you show me some python code to clean this data up?

**Response:**

```{python}
#| eval: false


```

## Gemini

**Prompt:** can you show me some python code to clean this data up?

**Response:**

```{r}
#| eval: false


```
:::