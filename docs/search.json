[
  {
    "objectID": "solutions.html",
    "href": "solutions.html",
    "title": "Solutions",
    "section": "",
    "text": "Important\n\n\n\nNote that LLMs are stochastic/non-deteriministic, so there are no definitive solutions."
  },
  {
    "objectID": "solutions.html#possible-solutions-to-exercises",
    "href": "solutions.html#possible-solutions-to-exercises",
    "title": "Solutions",
    "section": "Possible Solutions to Exercises",
    "text": "Possible Solutions to Exercises\n\nExercise 1\nPrompt: Use an LLM to clean data in preparation for analysis and modeling.\nPossible Solutions\n\nChatGPTClaudeGemini\n\n\nPrompt: You are Dr. Karandeep Singh, the creator of TidierOrg and Tidier.jl. You are an expert in coding, AI, and nephrology.\n[Objective] Clean messy clinical trial data to prepare for analysis and predictive modeling.\n[Context] Take this messy data {messy_aki.xlsx} and provide valid Julia code to accurately clean the data.\n[Constraints] You must use TidierOrg tools to clean the data.\n[Format] Return valid Julia code to read in the messy data, clean and validate the data, and export the data to a csv. Provide a report in markdown that catalogs missing data, outliers, and any edge cases that need HITL review before modeling. Clearly explain which Tidier.jl packages you are using and what each section of the code achieves with the resulting Julia code you generate. Do this in a manner for someone who is familiar with code but knows nothing about Julia. Provide key points for choosing Julia over other programming languages.\nResponse:\nJulia cleaning script (TidierOrg-first)\n\n############################################################\n# AKI trial data cleaning (TidierOrg / Tidier.jl workflow)\n# - Reads messy Excel\n# - Detects the true header row\n# - Cleans/standardizes fields (dates, categories, numerics)\n# - Validates + flags HITL edge cases\n# - Exports clean CSV + a markdown QA report\n############################################################\n\nusing Tidier                    # meta-package (re-exports TidierData, TidierFiles, TidierStrings, TidierDates, etc.)\nusing DataFrames\nusing Dates\nusing Statistics\n\n# -------------------------\n# 0) I/O paths\n# -------------------------\nin_path  = \"/mnt/data/messy_aki.xlsx\"\nout_csv  = \"aki_clean.csv\"\nout_md   = \"aki_cleaning_report.md\"\n\n# -------------------------\n# 1) Read Excel (TidierFiles)\n#    We read with col_names=false because the file has title rows above the real header\n# -------------------------\ndf_raw = TidierFiles.read_xlsx(in_path; sheet=1, col_names=false, trim_ws=true)\n\n# -------------------------\n# 2) Find the header row containing \"study_id\" in the first column, then re-read as data\n# -------------------------\nfirstcol = df_raw[:, 1]\nheader_row = findfirst(x -&gt; !ismissing(x) && lowercase(String(x)) == \"study_id\", firstcol)\n\nif header_row === nothing\n    error(\"Could not find a header row with 'study_id' in column 1. Inspect df_raw to locate headers.\")\nend\n\n# Extract header values and data rows\nhdr_vals = df_raw[header_row, :]\ndf = df_raw[(header_row + 1):nrow(df_raw), :]\n\n# Assign column names from the header row\nrename!(df, Symbol.(map(x -&gt; ismissing(x) ? \"\" : String(x), collect(hdr_vals))))\n\n# Drop any columns with empty names (common when there are blank columns in the sheet)\nempty_name_cols = names(df)[map(nm -&gt; nm == Symbol(\"\"), names(df))]\nif !isempty(empty_name_cols)\n    select!(df, Not(empty_name_cols))\nend\n\n# Clean column names to snake_case (TidierData)\ndf = @chain df begin\n    @clean_names()\nend\n\n# -------------------------\n# 3) Drop trailing note rows / non-data rows\n#    (in this file, rows with missing study_id are notes/permissions)\n# -------------------------\ndf = @chain df begin\n    @filter(!ismissing(study_id))\nend\n\n# -------------------------\n# 4) Helper parsers (used inside @mutate)\n# -------------------------\n# Extract the first number from messy strings like \"19 but poor collection\"\nfunction first_number(x)\n    if ismissing(x)\n        return missing\n    end\n    if x isa Number\n        return Float64(x)\n    end\n    s = strip(String(x))\n    m = match(r\"-?\\d+(\\.\\d+)?\", s)\n    return m === nothing ? missing : parse(Float64, m.match)\nend\n\n# Parse DOB from multiple common Excel/string formats\nfunction parse_dob(x)\n    if ismissing(x)\n        return missing\n    end\n    # Already a Date/DateTime\n    if x isa Date\n        return x\n    elseif x isa DateTime\n        return Date(x)\n    end\n\n    s = strip(String(x))\n    if isempty(s)\n        return missing\n    end\n\n    # Normalize month-name strings like \"may 5 1970\" -&gt; \"May 5 1970\"\n    s2 = join(uppercasefirst.(split(lowercase(s), ' ')), ' ')\n\n    # Try a few formats\n    for fmt in (dateformat\"y-m-d\", dateformat\"m/d/y\", dateformat\"u d y\", dateformat\"u d, y\")\n        try\n            return Date(s2, fmt)\n        catch\n        end\n    end\n\n    # Last resort: try Date(s2) (can work for some ISO-ish strings)\n    try\n        return Date(s2)\n    catch\n        return missing\n    end\nend\n\n# Standardize Yes/No-ish fields\nfunction yesno(x)\n    if ismissing(x)\n        return missing\n    end\n    s = lowercase(strip(String(x)))\n    if s in (\"yes\", \"y\", \"true\", \"1\")\n        return \"Yes\"\n    elseif s in (\"no\", \"n\", \"false\", \"0\")\n        return \"No\"\n    else\n        return missing\n    end\nend\n\n# Standardize sex\nfunction sex_std(x)\n    if ismissing(x)\n        return missing\n    end\n    s = lowercase(strip(String(x)))\n    if s in (\"m\", \"male\")\n        return \"Male\"\n    elseif s in (\"f\", \"female\")\n        return \"Female\"\n    else\n        return missing\n    end\nend\n\n# Standardize race (example mapping—edit if your protocol uses different groupings)\nfunction race_std(x)\n    if ismissing(x)\n        return missing\n    end\n    s = lowercase(strip(String(x)))\n    if s in (\"white\", \"caucasian\", \"w\", \"white \")\n        return \"White\"\n    elseif s in (\"black\", \"af-am\", \"african american\", \"african-american\")\n        return \"Black\"\n    elseif s in (\"asian\")\n        return \"Asian\"\n    elseif s in (\"mixed\", \"multiracial\")\n        return \"Mixed\"\n    elseif s in (\"other\")\n        return \"Other\"\n    else\n        return \"Other\"\n    end\nend\n\n# Standardize Hispanic ethnicity with typo handling\nfunction hispanic_std(x)\n    if ismissing(x)\n        return missing\n    end\n    s = lowercase(strip(String(x)))\n    if s in (\"hispanic\", \"h\", \"h \", \"hisspanic\", \"hispannic\")\n        return \"Hispanic\"\n    elseif s in (\"not hispanic\", \"nh\", \"non-hispanic\", \"non hispanic\")\n        return \"Not Hispanic\"\n    else\n        return missing\n    end\nend\n\n# -------------------------\n# 5) Main cleaning transforms (TidierData)\n#    - Parse types\n#    - Keep “raw” copies where useful for audit\n#    - Create QA flags for HITL review\n# -------------------------\ndf_clean = @chain df begin\n    @mutate(\n        # IDs\n        study_id = as_integer(study_id),\n\n        # DOB -&gt; Date\n        dob_raw = dob,\n        dob = ~parse_dob(dob),\n\n        # Demographics standardization\n        sex_raw = sex,\n        sex = ~sex_std(sex),\n\n        race_raw = race,\n        race = ~race_std(race),\n\n        hispanic_raw = hispanic,\n        hispanic = ~hispanic_std(hispanic),\n\n        # GFR numerics (keep raw columns + parsed numeric columns)\n        admission_gfr_raw = admission_gfr,\n        day_1_gfr_raw      = day_1_gfr,\n        day2gfr_raw        = day2gfr,\n        day_3_gfr_raw      = day_3_gfr,\n        day_4_gfr_raw      = day_4_gfr,\n        day5_gfr_raw       = day5_gfr,\n\n        admission_gfr = ~first_number(admission_gfr),\n        day_1_gfr     = ~first_number(day_1_gfr),\n        day2_gfr      = ~first_number(day2gfr),\n        day_3_gfr     = ~first_number(day_3_gfr),\n        day_4_gfr     = ~first_number(day_4_gfr),\n        day5_gfr      = ~first_number(day5_gfr),\n\n        # Flags: “had extra text” (e.g., \"19 but poor collection\")\n        day2_parse_note = !(ismissing(day2gfr_raw)) && !(String(day2gfr_raw) |&gt; strip |&gt; x -&gt; occursin(r\"^-?\\d+(\\.\\d+)?$\", x)),\n\n        # Devices\n        hd_catheter_raw = hd_catheter,\n        hd_catheter = ~yesno(hd_catheter),\n\n        av_fistula_raw = av_fistula,\n        av_fistula = ~yesno(av_fistula)\n    ) |&gt;\n\n    # Keep a tidy column order (optional)\n    @select(\n        study_id, dob, sex, race, hispanic,\n        admission_gfr, day_1_gfr, day2_gfr, day_3_gfr, day_4_gfr, day5_gfr,\n        diagnosis, sediment, hd_catheter, av_fistula,\n        day2_parse_note,\n        # audit columns at the end\n        dob_raw, sex_raw, race_raw, hispanic_raw,\n        admission_gfr_raw, day_1_gfr_raw, day2gfr_raw, day_3_gfr_raw, day_4_gfr_raw, day5_gfr_raw,\n        hd_catheter_raw, av_fistula_raw\n    )\nend\n\n# -------------------------\n# 6) Validation + HITL flags\n# -------------------------\n\n# Duplicated study_id check\ndup_ids = @chain df_clean begin\n    @count(study_id)\n    @filter(n &gt; 1)\nend\n\n# Missingness summary\nmissing_summary = DataFrame(\n    column = String[],\n    n_missing = Int[],\n    pct_missing = Float64[]\n)\nfor nm in names(df_clean)\n    col = df_clean[!, nm]\n    nmiss = count(ismissing, col)\n    push!(missing_summary, (String(nm), nmiss, round(100 * nmiss / nrow(df_clean); digits=1)))\nend\n\n# Outlier thresholds (IQR rule) for each GFR column\ngfr_cols = [:admission_gfr, :day_1_gfr, :day2_gfr, :day_3_gfr, :day_4_gfr, :day5_gfr]\n\nfunction iqr_bounds(v::Vector{Union{Missing, Float64}})\n    x = collect(skipmissing(v))\n    if length(x) &lt; 4\n        return (missing, missing)  # too few points\n    end\n    q1 = quantile(x, 0.25)\n    q3 = quantile(x, 0.75)\n    iqr = q3 - q1\n    return (q1 - 1.5*iqr, q3 + 1.5*iqr)\nend\n\noutlier_rows = DataFrame()\nfor c in gfr_cols\n    lo, hi = iqr_bounds(df_clean[!, c])\n    if !(ismissing(lo) || ismissing(hi))\n        tmp = @chain df_clean begin\n            @mutate(_col = String(c), _lo = lo, _hi = hi)\n            @filter(!ismissing($(c)) && (($(c) &lt; lo) || ($(c) &gt; hi)))\n            @select(study_id, _col, $(c), _lo, _hi)\n        end\n        outlier_rows = vcat(outlier_rows, tmp; cols=:union)\n    end\nend\n\n# Extra clinical sanity flags (edit thresholds to your protocol)\nsanity_flags = @chain df_clean begin\n    @mutate(\n        flag_dob_unparsed = ismissing(dob),\n        flag_gfr_negative = any.((admission_gfr, day_1_gfr, day2_gfr, day_3_gfr, day_4_gfr, day5_gfr) .&lt; 0),\n        flag_gfr_very_high = any.((admission_gfr, day_1_gfr, day2_gfr, day_3_gfr, day_4_gfr, day5_gfr) .&gt; 200),\n        flag_day3_missing = ismissing(day_3_gfr),\n        flag_day2_note = day2_parse_note\n    )\nend\n\n# -------------------------\n# 7) Export clean CSV (TidierFiles)\n# -------------------------\nTidierFiles.write_csv(df_clean, out_csv)\n\n# -------------------------\n# 8) Write a markdown QA report\n# -------------------------\nfunction df_to_md_table(d::DataFrame; max_rows::Int=50)\n    n = min(nrow(d), max_rows)\n    cols = names(d)\n    header = \"|\" * join(string.(cols), \"|\") * \"|\\n\"\n    sep    = \"|\" * join(fill(\"---\", length(cols)), \"|\") * \"|\\n\"\n    rows = \"\"\n    for i in 1:n\n        vals = [ismissing(d[i, c]) ? \"missing\" : string(d[i, c]) for c in cols]\n        rows *= \"|\" * join(vals, \"|\") * \"|\\n\"\n    end\n    if nrow(d) &gt; max_rows\n        rows *= \"\\n*(truncated to first $(max_rows) rows)*\\n\"\n    end\n    return header * sep * rows\nend\n\nreport = \"\"\"\n# AKI Data Cleaning QA Report\n\n## Files\n- Input: `$(in_path)`\n- Output CSV: `$(out_csv)`\n\n## Row counts\n- Rows after header detection + dropping note rows: **$(nrow(df_clean))**\n- Columns: **$(ncol(df_clean))**\n\n## Missing data (all columns)\n$(df_to_md_table(sort(missing_summary, :n_missing, rev=true)))\n\n## Duplicate study IDs (requires HITL)\n$(nrow(dup_ids) == 0 ? \"_None detected._\" : df_to_md_table(dup_ids))\n\n## Parsed-text notes (e.g., GFR cell had extra text)\nRows with `day2_parse_note = true` should be reviewed to confirm the numeric extraction matches intent.\n$(df_to_md_table(@chain df_clean begin\n    @filter(day2_parse_note == true)\n    @select(study_id, day2gfr_raw, day2_gfr)\nend))\n\n## Outliers by IQR rule (requires HITL)\n$(nrow(outlier_rows) == 0 ? \"_None detected by IQR rule._\" : df_to_md_table(outlier_rows))\n\n## Sanity flags (requires HITL)\n$(df_to_md_table(@chain sanity_flags begin\n    @filter(flag_dob_unparsed || flag_gfr_very_high || flag_day3_missing || flag_day2_note)\n    @select(study_id, flag_dob_unparsed, flag_gfr_very_high, flag_day3_missing, flag_day2_note,\n            admission_gfr, day_1_gfr, day2_gfr, day_3_gfr, day_4_gfr, day5_gfr,\n            dob_raw)\nend))\n\"\"\"\n\nopen(out_md, \"w\") do io\n    write(io, report)\nend\n\nprintln(\"Wrote clean CSV -&gt; $(out_csv)\")\nprintln(\"Wrote QA report -&gt; $(out_md)\")\n\nWhat TidierOrg packages this uses (and why)\n\nTidier.jl (meta-package): loads the “tidyverse-like” ecosystem in one import. Tidier.jl is designed as a meta-package that re-exports functionality from smaller TidierOrg packages.\nTidierFiles.jl: provides read_xlsx() and write_csv() with a consistent “readr/readxl-ish” API, powered by XLSX.jl/CSV.jl under the hood.\nTidierData.jl: provides tidy macros like @chain, @mutate, @filter, @select, and @clean_names() for dplyr/tidyr-like transformations.\n\nMarkdown QA report (based on this messy_aki.xlsx)\nSummary\n\nExcel rows: 27 total on the sheet\nTrue data table rows: 23 (after skipping title/header junk)\nNon-data note rows removed: 3 (permission/notes at bottom with missing study_id)\nFinal analytic rows: 20\nKey HITL items: duplicate study IDs, extreme/implausible GFR values, one missing Day 3 GFR, “numeric-with-text” cell.\n\nMissing data\n\nday_3_gfr: 1 missing (study_id 21)\n\nOutliers (needs HITL review)\nUsing an IQR rule on the numeric-parsed GFR values, these stand out:\n\nAdmission GFR = 232 (study_id 20) — likely a data entry error (e.g., “23.2” or “23”?) given subsequent days are ~19–33.\nAdmission GFR = 177 (study_id 21) — unusually high vs cohort; may be real or a typo (“17.7” / “17”?).\nDay 1 GFR = 322 (study_id 15) — almost certainly a typo (pattern suggests “32”).\n\nEdge cases / messy-field normalization\n\nDOB includes a free-text date (\"may 5 1970\") that must be parsed carefully (the script handles multiple formats).\nSex mixes Male/Female with M/F and trailing whitespace.\nRace mixes casing and abbreviations (WHITE, Af-Am, Caucasian, etc.).\nHispanic includes typos (Hisspanic, Hispannic, H) and shorthand (NH).\nDay2 GFR contains at least one cell like \"19 but poor collection\"; the script extracts the first numeric value (=19) and flags it (day2_parse_note=true) for review.\n\nID integrity (must resolve before modeling)\n\nDuplicate study_id: 3 appears twice; 8 appears twice.\nThese represent clearly different patients/records and should be reconciled (true duplicates vs mis-keyed IDs) before train/test splits.\n\nHow to read the Julia code if you “know code but not Julia”\n\nusing Tidier is like library(tidyverse)—it brings in the TidierOrg ecosystem.\n@chain df begin ... end is like %&gt;% in R or method-chaining in Python; it pipes a DataFrame through steps.\n@mutate(new = f(old)) creates/overwrites columns.\nmissing is Julia’s NA.\nThe ~ prefix inside TidierData macros tells TidierData not to auto-vectorize a function (important when you’re calling helper functions that should run “per-cell” rather than being broadcast in a surprising way).\n\nWhy choose Julia here (vs Python/R) — the practical version\n\nSpeed without giving up expressiveness: you can do heavy feature engineering and modeling in one language without “two-language” rewrites.\nComposable, high-performance tabular + ML stack: DataFrames + MLJ/Flux/etc. can stay fast as the pipeline grows.\nTidyverse-like ergonomics (TidierOrg): if your team thinks in dplyr verbs, TidierData lowers the adoption cost while staying Julia-native.\n\nIf you want, I can adapt the race/ethnicity mappings and the GFR plausibility thresholds to match your trial’s data dictionary / SAP (e.g., “Unknown” handling, allowable ranges, and whether GFR is eGFR vs measured clearance).\n\n\nPrompt: can you show me some python code to clean this data up?\nResponse:\nHere’s a well-commented cleaning script that tackles every issue I spotted:\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom dateutil import parser as dateparser\nimport re\n\n# ── 1. Load, skip the 3-row title block, drop the empty column F ──────────────\ndf = pd.read_excel(\"messy_aki.xlsx\", skiprows=4, header=0)\ndf = df.drop(columns=[df.columns[5]])          # unnamed empty column between Hispanic and Admission GFR\ndf = df.dropna(how=\"all\")                       # drop fully-empty rows (incl. footer notes)\n\n# ── 2. Standardise column names ───────────────────────────────────────────────\ndf.columns = [\n    \"study_id\", \"dob\", \"race\", \"sex\", \"hispanic\",\n    \"gfr_admission\", \"gfr_day1\", \"gfr_day2\", \"gfr_day3\", \"gfr_day4\", \"gfr_day5\",\n    \"diagnosis\", \"sediment\", \"hd_catheter\", \"av_fistula\"\n]\n\n# ── 3. Duplicate / invalid study IDs ─────────────────────────────────────────\n# IDs 3 and 8 appear twice; IDs 9, 10, 18 are missing entirely.\n# Flag duplicates so you can review — don't silently drop them.\ndf[\"duplicate_id_flag\"] = df.duplicated(subset=\"study_id\", keep=False)\n\n# ── 4. Parse dates of birth ───────────────────────────────────────────────────\ndef parse_dob(val):\n    if pd.isna(val):\n        return pd.NaT\n    if isinstance(val, pd.Timestamp):\n        return val\n    # Handle obvious typo like \"5=11-1984\"\n    val = str(val).replace(\"=\", \"-\")\n    try:\n        return dateparser.parse(val, dayfirst=False)\n    except Exception:\n        return pd.NaT\n\ndf[\"dob\"] = pd.to_datetime(df[\"dob\"].apply(parse_dob), errors=\"coerce\")\n\n# ── 5. Standardise race ───────────────────────────────────────────────────────\nrace_map = {\n    \"caucasian\": \"White\",\n    \"white\":     \"White\",\n    \"af-am\":     \"Black\",\n    \"black\":     \"Black\",\n    \"asian\":     \"Asian\",\n    \"mixed\":     \"Other\",\n    \"other\":     \"Other\",\n}\ndf[\"race\"] = (df[\"race\"]\n              .str.strip()\n              .str.lower()\n              .map(race_map)\n              .fillna(\"Unknown\"))\n\n# ── 6. Standardise sex ────────────────────────────────────────────────────────\ndef clean_sex(val):\n    if pd.isna(val):\n        return np.nan\n    v = str(val).strip().lower()\n    if v in (\"male\", \"m\"):\n        return \"Male\"\n    if v in (\"female\", \"f\"):\n        return \"Female\"\n    return np.nan\n\ndf[\"sex\"] = df[\"sex\"].apply(clean_sex)\n\n# ── 7. Standardise hispanic ethnicity ────────────────────────────────────────\ndef clean_hispanic(val):\n    if pd.isna(val):\n        return np.nan\n    v = str(val).strip().lower().replace(\" \", \"\")\n    if v in (\"hispanic\", \"h\", \"hispannic\", \"hisspanic\"):   # typos\n        return \"Hispanic\"\n    if v in (\"nothispanic\", \"nh\"):\n        return \"Not Hispanic\"\n    return np.nan\n\ndf[\"hispanic\"] = df[\"hispanic\"].apply(clean_hispanic)\n\n# ── 8. Clean GFR columns — coerce text to numeric, flag outliers ──────────────\ngfr_cols = [\"gfr_admission\", \"gfr_day1\", \"gfr_day2\", \"gfr_day3\", \"gfr_day4\", \"gfr_day5\"]\n\ndef extract_numeric(val):\n    \"\"\"Pull the first number out of a cell; return NaN if none found.\"\"\"\n    if pd.isna(val):\n        return np.nan\n    try:\n        return float(val)\n    except (ValueError, TypeError):\n        match = re.search(r\"\\d+\\.?\\d*\", str(val))\n        return float(match.group()) if match else np.nan\n\nfor col in gfr_cols:\n    df[col] = df[col].apply(extract_numeric)\n\n# Flag physiologically implausible GFR values (&gt; 200 are likely data-entry errors)\nGFR_MAX = 200\nfor col in gfr_cols:\n    flag_col = col + \"_outlier_flag\"\n    df[flag_col] = df[col] &gt; GFR_MAX\n\n# ── 9. Standardise Yes/No columns ────────────────────────────────────────────\ndef clean_yesno(val):\n    if pd.isna(val):\n        return np.nan\n    v = str(val).strip().lower().replace(\"`\", \"\")   # \"No`\" typo\n    if v == \"yes\":\n        return \"Yes\"\n    if v == \"no\":\n        return \"No\"\n    return np.nan\n\nfor col in [\"hd_catheter\", \"av_fistula\"]:\n    df[col] = df[col].apply(clean_yesno)\n\n# ── 10. Standardise sediment free-text (minor normalisation) ─────────────────\ndf[\"sediment\"] = (df[\"sediment\"]\n                  .str.strip()\n                  .str.lower()\n                  .str.replace(r\"\\s+\", \" \", regex=True)\n                  .str.replace(\"epi cell\", \"epithelial cell\")\n                  .str.replace(\"gran \", \"granular \"))\n\n# ── 11. Report a cleaning summary ─────────────────────────────────────────────\nprint(\"=== Cleaning summary ===\")\nprint(f\"Rows: {len(df)}\")\nprint(f\"\\nDuplicate study IDs:\\n{df[df['duplicate_id_flag']][['study_id','dob','diagnosis']]}\")\nprint(f\"\\nUnparseable DOBs: {df['dob'].isna().sum()}\")\nprint(f\"\\nGFR outlier flags:\")\nfor col in gfr_cols:\n    n = df[col + \"_outlier_flag\"].sum()\n    if n:\n        print(f\"  {col}: {n} outlier(s) — {df.loc[df[col+'_outlier_flag'], ['study_id', col]].values.tolist()}\")\n\nprint(f\"\\nSex nulls after cleaning:      {df['sex'].isna().sum()}\")\nprint(f\"Hispanic nulls after cleaning: {df['hispanic'].isna().sum()}\")\nprint(f\"Race unknowns:                 {(df['race']=='Unknown').sum()}\")\n\n# ── 12. Save ──────────────────────────────────────────────────────────────────\ndf.to_csv(\"aki_clean.csv\", index=False)\nprint(\"\\nSaved to aki_clean.csv\")\n\n\n=== Cleaning summary ===\nRows: 22\n\nDuplicate study IDs:\n    study_id        dob           diagnosis\n2        3.0 1970-05-05        bleeding out\n3        3.0 1981-06-03  norovirus diarrhea\n8        8.0 1982-04-22     C diff diarrhea\n9        8.0 2003-10-12           NSAID tox\n21       NaN        NaT                 NaN\n22       NaN        NaT                 NaN\n\nUnparseable DOBs: 2\n\nGFR outlier flags:\n  gfr_admission: 1 outlier(s) — [[20.0, 232.0]]\n  gfr_day1: 1 outlier(s) — [[15.0, 322.0]]\n\nSex nulls after cleaning:      2\nHispanic nulls after cleaning: 2\nRace unknowns:                 2\n\nSaved to aki_clean.csv\nHere’s a rundown of every issue it handles:\nStructure — skips the 3-row title block (“Acute Kidney Injury Study”, resident name, project name) and the stray empty column between Hispanic and Admission GFR.\nColumn names — normalises the wildly inconsistent headers (DAY2GFR\\``,day 3 GfR`, etc.) to snake_case.\nDuplicate IDs — study_ids 3 and 8 each appear twice (and 9, 10, 18 are missing). The script flags duplicates rather than silently dropping them, since you’ll need to decide which row is correct.\nDates of birth — handles three formats: Excel dates, “may 5 1970” (free text), and “5=11-1984” (typo with = instead of -).\nRace — merges “Caucasian”/“WHITE”/“white” → “White”, “Af-Am”/“Black” → “Black”, etc.\nSex — normalises “M”, “F”, “Male”, “Female”, “F” → consistent “Male”/“Female”.\nHispanic — fixes typos (“Hispannic”, “Hisspanic”) and abbreviations (“H”, “NH”).\nGFR values — row 2 has “19 but poor collection” (extracts 19), and several rows have values like 322 or 232 that are almost certainly data-entry errors (extra digit). Those are flagged with outlier columns.\nYes/No fields — handles “YES”, “NO”, “No`” (backtick typo) → “Yes”/“No”.\n\n\nPrompt: can you show me some python code to clean this data up?\nResponse:\n#| eval: false\n\n\n\n\n\n\nExercise 2\nPrompt: Use an LLM to clean data in preparation for analysis and modeling.\nPossible Solutions\n\nChatGPTClaudeGemini\n\n\nPrompt: can you show me some python code to clean this data up?\n\nResponse:\n#| eval: false\n\n\n\nPrompt: you are an expert python coder. Using polars and scikitlearn, write code to predict remission status (1=remission) using a dataset with the following variables:\nVariables\ndays_of_life - age in days. Numeric. Range: 1207-32356. 1 missing value. plt - Platelet Count. Numeric. Range: 11-1114. 4 missing values. mpv - Mean Platelet Volume. Numeric. Range: 5.3-13.5. 21 missing values. un - Blood Urea Nitrogen. Numeric. Range: 2-118. 53 missing values. wbc - White Blood Cell Count. Numeric. Range: 0.7-33.5. No missing values. hgb - Hemoglobin. Numeric. Range: 4.5-18.6. 4 missing values. hct Hematocrit. Numeric. Range: 13.7-55.2. 3 missing values. rbc - Red Blood Cell Count. Numeric. Range: 1.57-7.04. 3 missing values. mcv - Mean Corpuscular (RBC) Volume. Numeric. Range: 56.5-124. 3 missing values. mch - Mean Corpuscular (RBC) Hemoglobin. Numeric. Range: 16.7-42.3. 7 missing values. mchc - Mean Corpuscular (RBC) Hemoglobin per Cell. Numeric. Range: 28.2-38.0. 7 missing values. rdw - Red cell Distribution Width. Numeric. Range: 11.3-39.7. 3 missing values. neut_percent - Percent of Neutrophils in WBC count. Numeric. Range: 17-98.1. No missing values. lymph_percent - Percent of Lymphocytes in WBC count. Numeric. Range: 1-67.9. No missing values. mono_percent - Percent of Monocytes in WBC count. Numeric. Range: 0-30.3. No missing values. eos_percent - Percent of Eosinophils in WBC count. Numeric. Range: 0.5-29.3. 6 missing values. baso_percent - Percent of Basoophils in WBC count. Numeric. Range: 0.2-5.3. 6 missing values. sod - Sodium. Numeric. Range: 116-151. No missing values. pot - Potassium. Numeric. Range: 2.6-10.1. 1 missing value. chlor - Chloride. Numeric. Range: 83-126. No missing values. co2 - Bicarbonate (CO2). Numeric. Range: 12-40. 5 missing values. creat - Creatinine. Numeric. Range: 0.2-8.4. No missing values. gluc - Glucose. Numeric. Range: 41-486. No missing values. cal - Calcium. Numeric. Range: 6.5-11.8. 1 missing value. prot - Protein. Numeric, range 2.9-10, 0 missing values alb - Albumin. Numeric, range 1.2-5.5, 0 missing values ast - Aspartate Transaminase. Numeric, range 5-7765, 0 missing values alt - Alanine Transaminase. Numeric, range 1-10666, 18 missing values alk - Alkaline phosphatase. Numeric, range 13-1938, 0 missing values tbil - Total Bilirubin. Numeric, range 0.09-27, 0 missing values active - Active Inflammation despite Thiopurines for &gt; 12 weeks. Numeric, range 0-1, 0 missing values remission - Remission of Inflammation after Thiopurines for &gt; 12 weeks. Numeric, range 0-1, 0 missing values\nResponse:\nHere’s a complete script using Polars and scikit-learn to predict remission status:\n\n\nCode\nimport polars as pl\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    classification_report, roc_auc_score, average_precision_score,\n    make_scorer, f1_score\n)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ── 1. Load data ──────────────────────────────────────────────────────────────\ndf = pl.read_csv(\"your_data.csv\")          # ← adjust path / format as needed\n\n# ── 2. Define features & target ───────────────────────────────────────────────\nTARGET = \"remission\"\n\nFEATURES = [\n    \"days_of_life\", \"plt\", \"mpv\", \"un\", \"wbc\", \"hgb\", \"hct\", \"rbc\",\n    \"mcv\", \"mch\", \"mchc\", \"rdw\",\n    \"neut_percent\", \"lymph_percent\", \"mono_percent\", \"eos_percent\", \"baso_percent\",\n    \"sod\", \"pot\", \"chlor\", \"co2\", \"creat\", \"gluc\", \"cal\",\n    \"prot\", \"alb\", \"ast\", \"alt\", \"alk\", \"tbil\",\n    \"active\",        # clinical flag — strong prior candidate\n]\n\n# ── 3. Polars preprocessing ───────────────────────────────────────────────────\n# Log-transform right-skewed lab values to reduce outlier influence\nskewed = [\"ast\", \"alt\", \"alk\", \"tbil\", \"gluc\", \"wbc\", \"plt\", \"un\", \"creat\"]\n\ndf = df.with_columns([\n    pl.col(c).log1p().alias(c) for c in skewed if c in df.columns\n])\n\n# Extract numpy arrays (Polars → numpy for sklearn)\nX = df.select(FEATURES).to_numpy().astype(np.float64)\ny = df[TARGET].to_numpy().astype(int)\n\nprint(f\"Dataset shape : {X.shape}\")\nprint(f\"Class balance : {np.bincount(y)} (0=no remission, 1=remission)\")\n\n# ── 4. Build pipelines ────────────────────────────────────────────────────────\n# Median imputation handles the missing values described above; scaler for LR\nimputer  = SimpleImputer(strategy=\"median\")\nscaler   = StandardScaler()\n\npipelines = {\n    \"Logistic Regression\": Pipeline([\n        (\"impute\", imputer),\n        (\"scale\",  scaler),\n        (\"clf\",    LogisticRegression(max_iter=1000, class_weight=\"balanced\",\n                                      C=0.1, solver=\"lbfgs\")),\n    ]),\n    \"Random Forest\": Pipeline([\n        (\"impute\", imputer),\n        (\"clf\",    RandomForestClassifier(\n            n_estimators=500, max_features=\"sqrt\",\n            class_weight=\"balanced\", random_state=42, n_jobs=-1\n        )),\n    ]),\n    \"Gradient Boosting\": Pipeline([\n        (\"impute\", imputer),\n        (\"clf\",    GradientBoostingClassifier(\n            n_estimators=300, learning_rate=0.05,\n            max_depth=3, subsample=0.8, random_state=42\n        )),\n    ]),\n}\n\n# ── 5. Cross-validation ───────────────────────────────────────────────────────\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nscorers = {\n    \"roc_auc\"  : \"roc_auc\",\n    \"avg_prec\" : \"average_precision\",\n    \"f1\"       : make_scorer(f1_score, zero_division=0),\n    \"accuracy\" : \"accuracy\",\n}\n\nresults = {}\nfor name, pipe in pipelines.items():\n    cv_res = cross_validate(pipe, X, y, cv=cv, scoring=scorers,\n                            return_train_score=False)\n    results[name] = {k: cv_res[f\"test_{k}\"] for k in scorers}\n    print(f\"\\n{'─'*50}\")\n    print(f\"  {name}\")\n    print(f\"{'─'*50}\")\n    for metric, vals in results[name].items():\n        print(f\"  {metric:&lt;12}  {vals.mean():.3f}  ±  {vals.std():.3f}\")\n\n# ── 6. Feature importance (Random Forest) ────────────────────────────────────\nprint(\"\\n\\n── Feature Importance (Random Forest) ──\")\nrf_pipe = pipelines[\"Random Forest\"]\nrf_pipe.fit(X, y)\nimportances = rf_pipe.named_steps[\"clf\"].feature_importances_\n\nfeat_df = (\n    pl.DataFrame({\"feature\": FEATURES, \"importance\": importances})\n    .sort(\"importance\", descending=True)\n)\nprint(feat_df)\n\n# ── 7. Final model + hold-out evaluation (optional) ──────────────────────────\n# Uncomment if you have a separate test set:\n\n# df_test = pl.read_csv(\"test_data.csv\")\n# X_test  = df_test.select(FEATURES).to_numpy().astype(np.float64)\n# y_test  = df_test[TARGET].to_numpy().astype(int)\n#\n# best_pipe = pipelines[\"Random Forest\"]   # swap to whichever won CV\n# best_pipe.fit(X, y)\n# y_pred  = best_pipe.predict(X_test)\n# y_prob  = best_pipe.predict_proba(X_test)[:, 1]\n#\n# print(classification_report(y_test, y_pred, target_names=[\"No Remission\",\"Remission\"]))\n# print(f\"ROC-AUC : {roc_auc_score(y_test, y_prob):.3f}\")\n# print(f\"Avg Prec: {average_precision_score(y_test, y_prob):.3f}\")\n\n\nKey design decisions explained:\nMissing data — SimpleImputer(strategy=\"median\") handles all the missingness you described. Median is preferred over mean here because several lab values (AST, ALT, ALK, BUN) are heavily right-skewed.\nLog transforms — Applied via Polars before the sklearn pipeline for AST, ALT, ALK, tbil, glucose, WBC, platelets, BUN, and creatinine. These are notorious for extreme outliers in clinical labs that can destabilize linear models and impair tree splits.\nClass imbalance — class_weight=\"balanced\" is set for both Logistic Regression and Random Forest, which automatically up-weights the minority class. GBM handles this implicitly through its loss function but you can add scale_pos_weight if imbalance is severe.\nThree model types — Logistic Regression gives interpretable coefficients and a calibration baseline; Random Forest is robust to multicollinearity (which will be present among the CBC components like HGB/HCT/RBC); Gradient Boosting often wins on tabular clinical data.\nEvaluation metrics — ROC-AUC, Average Precision (PR-AUC), and F1 are all reported because accuracy alone is misleading with class imbalance.\nactive feature — Included intentionally since active inflammation is clinically the most direct predictor of remission failure; if you want a “lab-only” model, simply remove it from FEATURES.\n\n\nPrompt: can you show me some python code to clean this data up?\nResponse:\n#| eval: false\n\n\n\n\n\n\nExercise 3\nPrompt: Use an LLM to clean data in preparation for analysis and modeling.\nPossible Solutions\n\nChatGPTClaudeGemini\n\n\nPrompt: can you show me some python code to clean this data up?\nResponse:\n#| eval: false\n\n\n\n\nPrompt: can you show me some python code to clean this data up?\nResponse:\n\n\nPrompt: can you show me some python code to clean this data up?\nResponse:\n#| eval: false"
  },
  {
    "objectID": "literature.html",
    "href": "literature.html",
    "title": "Literature",
    "section": "",
    "text": "CJASN SeriesASN Task ForcePredictive ModelingDiet/NutritionResearchVisionClinical\n\n\n\nArtificial Intelligence and Machine Learning in Dialysis Ready for Prime Time?\nKotanko P, Zhang H, Wang Y  Clinical Journal of the American Society of Nephrology, 2023.  DOI: 10.2215/CJN.0000000000000089\n\n\nApplication of Natural Language Processing in Nephrology Research\nDouglas F, Chan L.  Clinical Journal of the American Society of Nephrology, 2023.  DOI: 10.2215/CJN.0000000000000118\n\n\n\n\nResponsible Use of Artificial Intelligence to Improve Kidney Care: A Statement from the American Society of Nephrology\nTangri N, Cheungpasitporn W, Crittenden SD, Fornoni A, Peralta CA, Singh K, Usvyat LA, Waterman AD  Journal of the American Society of Nephrology, 2025.  DOI: 10.1681/ASN.0000000929\n\n\n\n\nHome Dialysis Prediction Using Artificial Intelligence\nMonaghan CK, Willetts J, Han H, Chaudhuri S, Ficociello LH, Kraus MA, 3 Giles HE, Usvyat L, Turk J  Kidney Medicine, 2025.  DOI: 10.1016/j.xkme.2024.100949\n\n\nArtificial intelligence in kidney disease and dialysis: from data mining to clinical impact\nNeri L, Zhang H, Usvyat LA.  Current Opinion in Nephrology and Hypertension, 2026.  DOI: 10.1097/MNH.0000000000001132\n\n\nEvaluation of performance measures in predictive artificial intelligence models to support medical decisions: overview and guidance\nVan Calster B, Collins GS, Vickers AJ, Wynants L, Kerr KF, Barreñada L, Varoquaux G, Singh K, Moons KGM, Hernandez-Boussard T, Timmerman D, McLernon DJ, Smeden Mvan, Steyerberg EW  Lancet Digit Health, 2025.  DOI: /10.1016/j.landig.2025.100916\n\n\nProspective comparison of econometric, machine learning, and foundation models for forecasting emergency department boarding patients\nPoursoltan L, Cao J, Clay B, Trimble B, Adrid L, Pan J, Chua A, Bell J, Longhurst CA, Zhu K, Singh K  npj Health Systems, 2025.  DOI: 10.1038/s44401-025-00054-z\n\n\nEvaluation of electronic health record-integrated artificial intelligence chart review\nKahl NM, Frieden MJ, Pope ZR, Millen MM, Tolia VM, Chan TC, Longhurst CA, Singh K, You AX  npj Health Systems, 2026.  DOI: 10.1038/s44401-025-00064-x\n\n\nGeneralizability of an acute kidney injury prediction model across health systems\nCao J, Zhang X, Shahinian V, Yin H, Steffick D, Saran R, Crowley S, Mathis M, Nadkarni GN, Heung M, Singh K  Nature Machine Intelligence, 2022.  DOI: 10.1038/s42256-022-00563-8\n\n\nSystematic Review and Meta-Analysis of Machine Learning Models for Acute Kidney Injury Risk Classification\nCama-Olivares A, Braun C, Takeuchi T, O’Hagan EC, Kaiser KA, Ghazi L, Chen J, Forni LG, Kane-Gill SL, Ostermann M, Shickel B, Ninan J, Neyra JA  Journal of the American Society of Nephrology, 2025.  DOI: 10.1681/ASN.0000000702\n\n\nMulticenter Development and Validation of a Multimodal Deep Learning Model to Predict Moderate to Severe AKI\nKoyner JL, Martin J, Carey KA, Caskey J, Edelson DP, Mayampurath A, Dligach D, Afshar M, ChurpekMM  Clinical Journal of the American Society of Nephrology, 2025.  DOI: 10.2215/CJN.0000000695\n\n\n\n\nFrom bytes to bites: application of large language models to enhance nutritional recommendations\nBergling K, Wang LC, Shivakumar O, Nandorine Ban A, Moore LW, Ginsberg N, Kooman J, Duncan N, Kotanko P, Zhang H  Clinical Kidney Journal, 2025.  DOI: 10.1093/ckj/sfaf082\n\n\nApplication of ChatGPT to Support Nutritional Recommendations for Dialysis Patients – A Qualitative and Quantitative Evaluation\nWang L, Zhang H, Ginsberg N, Ban AN, Kooman JP, Kotanko P  Journal of Renal Nutrition, 2024.  DOI: 10.1053/j.jrn.2024.09.001\n\n\n\n\nPerformance of GPT-4 Vision on kidney pathology exam questions\nMiao J, Thongprayoon C, Cheungpasitporn W, Cornell LD  Am J Clin Pathol, 2024.  DOI: 10.1093/ajcp/aqae030\n\n\nThe dawn of multimodal artificial intelligence in nephrology\nShickel B, Bihorac A  Nature Reviews Nephrology, 2024.  DOI: 10.1038/s41581-023-00799-6\n\n\n\n\nIdentification of kidney-related medications using AI from self-captured pill images\nSheikh MS, Dreesman B, Barreto EF, Thongprayoon C, Miao J, Suppadungsuk S, Mao MA, Qureshi F, Pham JH, Craici IM, Kashani KB, Cheungpasitporn W  Ren Fail, 2024.  DOI: 10.1080/0886022X.2024.2402075\n\n\nImaging and spatially resolved mass spectrometry applications in nephrology\nGorman BL, Shafer CC, Ragi N, Sharma K, Neumann EK, Anderton CR  Nature Reviews Nephrology, 2025.  DOI: 10.1038/s41581-025-00946-1\n\n\nNon-invasive biopsy diagnosis of diabetic kidney disease via deep learning applied to retinal images: a population-based study\nMeng Z, Guan Z, Yu S, Wu Y, Zhao Y, Shen J, et al.  The Lancet Digital Health, 2025.  DOI: 10.1016/j.landig.2025.02.008\n\n\n\n\nClinical Applications of Artificial Intelligence in Autosomal Dominant Polycystic Kidney Disease\nEbrahimi N, Cheungpasitporn W, Chebib FT, Borghol AHamid, Ghozloujeh ZGholizadeh, Norouzi S, Abdipour A  Nephrol Dial Transplant, 2026.  DOI: 10.1093/ndt/gfag010\n\n\nAI Scribes Are Not Productivity Tools (Yet)\nKim E, Liu VX, Singh K  NEJM, 2025.  DOI: 10.1056/AIe2501051\n\n\nArtificial Intelligence in Nephrology: Clinical Applications and Challenges\nSingh P, Goyal L, Mallick DC, Surani SR, Kaushik N, Chandramohan D, Simhadri PK  Kidney Medicine, 2025.  DOI: 10.1016/j.xkme.2024.100927"
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Note\n\n\n\nOptional: Use Google Colab to evaluate Python, Julia, and/or R code generated by the LLM. Select your programming language at Runtime &gt; Change runtime type &gt; [Python3 | R | Julia], paste the code in a cell, then click the Run Button. (NB: A Google account is required for Google Colab access.)"
  },
  {
    "objectID": "exercises.html#exercise-1-code-explanation",
    "href": "exercises.html#exercise-1-code-explanation",
    "title": "Exercises",
    "section": "Exercise 1: Code Explanation",
    "text": "Exercise 1: Code Explanation\nSearch for and copy code from GitHub and ask the LLM for a detailed explanation for what libraries and packages are being used and what code accomplishes."
  },
  {
    "objectID": "exercises.html#exercise-2-screening-for-clinical-trials",
    "href": "exercises.html#exercise-2-screening-for-clinical-trials",
    "title": "Exercises",
    "section": "Exercise 2: Screening for Clinical Trials",
    "text": "Exercise 2: Screening for Clinical Trials\nSearch for a kidney-related active trial on clinicaltrials.gov and copy the Participation Criteria. Ask the LLM to write code to identify patients in your database eligible for the trial.\nExample Trial Participation Criteria"
  },
  {
    "objectID": "exercises.html#exercise-3-visualization-and-predictive-modeling",
    "href": "exercises.html#exercise-3-visualization-and-predictive-modeling",
    "title": "Exercises",
    "section": "Exercise 3: Visualization and Predictive Modeling",
    "text": "Exercise 3: Visualization and Predictive Modeling\nUsing the following description of the clinical trial, copy and paste the variables and data descriptions into an LLM and ask it to: 1. Create a JAMA style Table 1 in Markdown. 2. Provide and explain code for building multiple predictive models for remission (remission = 1). 3. Optional: Download the dataset and evaluate the code in Google Colab.\nVariables\ndays_of_life - age in days. Numeric. Range: 1207-32356. 1 missing value.\nplt - Platelet Count. Numeric. Range: 11-1114. 4 missing values.\nmpv - Mean Platelet Volume. Numeric. Range: 5.3-13.5. 21 missing values.\nun - Blood Urea Nitrogen. Numeric. Range: 2-118. 53 missing values.\nwbc - White Blood Cell Count. Numeric. Range: 0.7-33.5. No missing values.\nhgb - Hemoglobin. Numeric. Range: 4.5-18.6. 4 missing values.\nhct Hematocrit. Numeric. Range: 13.7-55.2. 3 missing values.\nrbc - Red Blood Cell Count. Numeric. Range: 1.57-7.04. 3 missing values.\nmcv - Mean Corpuscular (RBC) Volume. Numeric. Range: 56.5-124. 3 missing values.\nmch - Mean Corpuscular (RBC) Hemoglobin. Numeric. Range: 16.7-42.3. 7 missing values.\nmchc - Mean Corpuscular (RBC) Hemoglobin per Cell. Numeric. Range: 28.2-38.0. 7 missing values.\nrdw - Red cell Distribution Width. Numeric. Range: 11.3-39.7. 3 missing values.\nneut_percent - Percent of Neutrophils in WBC count. Numeric. Range: 17-98.1. No missing values.\nlymph_percent - Percent of Lymphocytes in WBC count. Numeric. Range: 1-67.9. No missing values.\nmono_percent - Percent of Monocytes in WBC count. Numeric. Range: 0-30.3. No missing values.\neos_percent - Percent of Eosinophils in WBC count. Numeric. Range: 0.5-29.3. 6 missing values.\nbaso_percent - Percent of Basoophils in WBC count. Numeric. Range: 0.2-5.3. 6 missing values.\nsod - Sodium. Numeric. Range: 116-151. No missing values.\npot - Potassium. Numeric. Range: 2.6-10.1. 1 missing value.\nchlor - Chloride. Numeric. Range: 83-126. No missing values.\nco2 - Bicarbonate (CO2). Numeric. Range: 12-40. 5 missing values.\ncreat - Creatinine. Numeric. Range: 0.2-8.4. No missing values.\ngluc - Glucose. Numeric. Range: 41-486. No missing values.\ncal - Calcium. Numeric. Range: 6.5-11.8. 1 missing value.\nprot - Protein. Numeric, range 2.9-10, 0 missing values\nalb - Albumin. Numeric, range 1.2-5.5, 0 missing values\nast - Aspartate Transaminase. Numeric, range 5-7765, 0 missing values\nalt - Alanine Transaminase. Numeric, range 1-10666, 18 missing values\nalk - Alkaline phosphatase. Numeric, range 13-1938, 0 missing values\ntbil - Total Bilirubin. Numeric, range 0.09-27, 0 missing values\nactive - Active Inflammation despite Thiopurines for &gt; 12 weeks. Numeric, range 0-1, 0 missing values\nremission - Remission of Inflammation after Thiopurines for &gt; 12 weeks. Numeric, range 0-1, 0 missing values\nDownload CSV\n\nDescription\nCodebook\nPaper\nCitation: Higgins P (2023). medicaldata: Data Package for Medical Datasets. https://higgi13425.github.io/medicaldata/, https://github.com/higgi13425/medicaldata/.\nExtension: Try using Paper Banana to create a visualization or CONSORT diagram."
  },
  {
    "objectID": "exercises.html#bonus-exercise-handling-messy-data",
    "href": "exercises.html#bonus-exercise-handling-messy-data",
    "title": "Exercises",
    "section": "Bonus Exercise: Handling Messy Data",
    "text": "Bonus Exercise: Handling Messy Data\nUse an LLM to clean data in preparation for analysis and modeling.\n\nDownload the messy_aki dataset from Dr. Peter Higgins’ {medicaldata} R package.\n\nPrompt an LLM to write code to clean the data.\nEvaluate the results and adjust your prompt to address any missed issues.\n\nData: messy_aki\nOptional Extension: Use the LLM to visualize eGFR trends for each patient over time."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "00_slides.html#data-and-generative-ai",
    "href": "00_slides.html#data-and-generative-ai",
    "title": "Data and Generative AI",
    "section": "Data and Generative AI",
    "text": "Data and Generative AI"
  },
  {
    "objectID": "00_slides.html#disclosures",
    "href": "00_slides.html#disclosures",
    "title": "Data and Generative AI",
    "section": "Disclosures",
    "text": "Disclosures\n\nLuca Neri, MD, PhD - TK\n\n\nKurtis A. Pivert - Employee, American Society of Nephrology"
  },
  {
    "objectID": "00_slides.html#the-best-of-times.",
    "href": "00_slides.html#the-best-of-times.",
    "title": "Data and Generative AI",
    "section": "The Best of Times….",
    "text": "The Best of Times….\n\nCan We Build It?\n\n\nYes"
  },
  {
    "objectID": "00_slides.html#the-worst-of-times",
    "href": "00_slides.html#the-worst-of-times",
    "title": "Data and Generative AI",
    "section": "…. The Worst of Times",
    "text": "…. The Worst of Times\n\nCan We Trust It?\n\n\n?\n\n\nJD Long\nR!sk 2026 Conference  February 18, 2026"
  },
  {
    "objectID": "00_slides.html#using-llms-for-data-analysis-recommendations",
    "href": "00_slides.html#using-llms-for-data-analysis-recommendations",
    "title": "Data and Generative AI",
    "section": "Using LLMs for Data Analysis: Recommendations",
    "text": "Using LLMs for Data Analysis: Recommendations\n\nTK\nTK\nTK"
  },
  {
    "objectID": "00_slides.html#security",
    "href": "00_slides.html#security",
    "title": "Data and Generative AI",
    "section": "Security",
    "text": "Security\n\nKnow your ToS\nDon’t allow use for model improvement\n\nChatGPT: Settings &gt; Data Controls &gt; Improve The Model for Everyone: Off\nClaude: Settings &gt; Privacy &gt; Help Improve Claude: Off\nGemini: Google Account &gt; Gemini Apps Activity &gt; Improve Google service: Unchecked\n\nEven when secure, avoid passing raw deidentified data to LLM"
  },
  {
    "objectID": "00_slides.html#options",
    "href": "00_slides.html#options",
    "title": "Data and Generative AI",
    "section": "Options",
    "text": "Options\n\n\nProvide column names and data types\n\n\n\n\nSynthetic data set"
  },
  {
    "objectID": "00_slides.html#exercise-1",
    "href": "00_slides.html#exercise-1",
    "title": "Data and Generative AI",
    "section": "Exercise 1",
    "text": "Exercise 1\nCode Explanation\nSearch for and copy code from GitHub and ask the LLM for a detailed explanation for what libraries and packages are being used and what code accomplishes."
  },
  {
    "objectID": "00_slides.html#exercise-2",
    "href": "00_slides.html#exercise-2",
    "title": "Data and Generative AI",
    "section": "Exercise 2",
    "text": "Exercise 2\nScreening for Clinical Trials\nSearch for a kidney-related active trial on clinicaltrials.gov and copy the Participation Criteria. Ask the LLM to write code to identify patients in your database eligible for the trial.\nExample Trial Participation Criteria"
  },
  {
    "objectID": "00_slides.html#exercise-3",
    "href": "00_slides.html#exercise-3",
    "title": "Data and Generative AI",
    "section": "Exercise 3",
    "text": "Exercise 3\nVisualization and Predictive Modeling\nUsing the following description of the clinical trial, copy and paste the variables and data descriptions into an LLM and ask it to:\n1. Create a JAMA style Table 1 in Markdown.\n2. Provide and explain code for building multiple predictive models for remission (remission = 1).\n3. Optional: Download the dataset and evaluate the code in Google Colab."
  },
  {
    "objectID": "00_slides.html#bonus-exercise",
    "href": "00_slides.html#bonus-exercise",
    "title": "Data and Generative AI",
    "section": "Bonus Exercise",
    "text": "Bonus Exercise\nHandling Messy Data\nUse an LLM to clean data in preparation for analysis and modeling.\n\nDownload the messy_aki dataset from Dr. Peter Higgins’ {medicaldata} R package.\nPrompt an LLM to write code to clean the data.\nEvaluate the results and adjust your prompt to address any missed issues."
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "Untitled",
    "section": "",
    "text": "At this year’s National Kidney Foundation Spring Clinical Meetings in Boston, Dr. @Lili Chan of the Barbara T. Murphy Division of Nephrology and the Windreich Department of Artificial Intelligence and Human Health at the Icahn School of Medicine at Mount Sinai gave a compelling presentation on the integration of artificial intelligence into routine kidney care.\nThe AI Revolution in Kidney Care\nDr. Chan began her session by polling the audience on their use of AI in clinical practice, highlighting how artificial intelligence is no longer a futuristic concept-it is increasingly embedded in the day-to-day workflows of healthcare. She organized her presentation around three primary learning objectives:\n\nUnderstanding Natural Language Processing (NLP) and Large Language Models (LLMs)\nReviewing machine learning (ML) applications in clinical care\nDiscussing risks and responsible approaches to AI implementation\n\nTo ground these concepts in clinical relevance, she presented a patient case journey-demonstrating how AI impacts multiple touchpoints, from patients Googling lab results to decision support in clinical encounters and follow-up planning.\nTechnical Foundations: Making AI Understandable\nDr. Chan provided an approachable explanation of AI’s inner workings, particularly for attendees less familiar with the technology. She described NLP as the subfield of AI that enables machines to analyze and understand human language, and LLMs as its most sophisticated tools-trained with vast datasets to generate and interpret medical text.\nShe illustrated the vectorization process-how computers convert language into numerical data for processing-using Word2Vec and BERT. The concept of attention mechanisms in transformer models was brought to life with a striking example of the word “bark,” which has entirely different meanings based on context. These demonstrations helped demystify how AI models contextualize medical language.\nAI’s Clinical Impact: Real-World Examples in Nephrology\nWhat set Dr. Chan’s presentation apart was her focus on actionable, real-world applications of AI:\n\nSymptom Detection in Dialysis: Her team showed that AI-driven analysis of clinical notes outperformed patient surveys in identifying symptoms, with sensitivity scores of 0.92 versus 0.58-highlighting AI’s potential to uncover unreported concerns in hemodialysis patients. \n\n\n\nAmbient AI Scribes: Data from the Permanente Medical Group demonstrated tangible workflow improvements, including reduced after-hours EHR use, faster documentation, and high-quality note generation (48 out of 50 on scoring metrics).\nAI in Pathology: Dr. Chan showcased cutting-edge developments in nephropathology, where AI is used to segment kidney structures, detect interstitial fibrosis, and even predict eGFR decline based on histomorphological data.\n\nPractical Implementation: Prompt Engineering and Use Cases\nFor clinicians ready to experiment with AI tools, Dr. Chan introduced best practices in prompt engineering. Her guidance included being specific and clear, setting expectations for tone and output format, and iterating through revisions for optimal results. She encouraged participants to try generating LLM-powered diet recommendations customized for kidney patients, as a hands-on exercise in refining AI prompts.\nBalanced Perspective: Ethical and Practical Cautions\nWhile optimistic about AI’s future in kidney care, Dr. Chan was careful to underscore the limitations and ethical concerns:\n\nPrivacy: AI systems often run in the cloud-meaning uploading protected health information (PHI) poses serious risks.\nHallucinations: AI may fabricate content or citations, which necessitates careful cross-verification by clinicians.\nImplementation Barriers: Key challenges include the opaque (“black box”) nature of many models, the need for informed consent, and ensuring AI tools are accessible to all populations.\n\n\n😊 Thank for for your wonderful talk, we really enjoyed learning from you @Lili Chan ; my friends/colleagues @Jing Miao @Muhammad Yasir Baloch and I were truly fascinated by your presentation.\nWhat are the most significant obstacles preventing frontline nephrologists from using AI tools regularly? → Time, cost, trust, or lack of training? Happy to hear discussions from you all @Lili Chan @Karin Bergling @Len Usvyat @Kim Solez"
  },
  {
    "objectID": "background.html#integrating-ai-into-everyday-kidney-care-highlights-from-nkf-2025-in-boston",
    "href": "background.html#integrating-ai-into-everyday-kidney-care-highlights-from-nkf-2025-in-boston",
    "title": "Untitled",
    "section": "",
    "text": "At this year’s National Kidney Foundation Spring Clinical Meetings in Boston, Dr. @Lili Chan of the Barbara T. Murphy Division of Nephrology and the Windreich Department of Artificial Intelligence and Human Health at the Icahn School of Medicine at Mount Sinai gave a compelling presentation on the integration of artificial intelligence into routine kidney care.\nThe AI Revolution in Kidney Care\nDr. Chan began her session by polling the audience on their use of AI in clinical practice, highlighting how artificial intelligence is no longer a futuristic concept-it is increasingly embedded in the day-to-day workflows of healthcare. She organized her presentation around three primary learning objectives:\n\nUnderstanding Natural Language Processing (NLP) and Large Language Models (LLMs)\nReviewing machine learning (ML) applications in clinical care\nDiscussing risks and responsible approaches to AI implementation\n\nTo ground these concepts in clinical relevance, she presented a patient case journey-demonstrating how AI impacts multiple touchpoints, from patients Googling lab results to decision support in clinical encounters and follow-up planning.\nTechnical Foundations: Making AI Understandable\nDr. Chan provided an approachable explanation of AI’s inner workings, particularly for attendees less familiar with the technology. She described NLP as the subfield of AI that enables machines to analyze and understand human language, and LLMs as its most sophisticated tools-trained with vast datasets to generate and interpret medical text.\nShe illustrated the vectorization process-how computers convert language into numerical data for processing-using Word2Vec and BERT. The concept of attention mechanisms in transformer models was brought to life with a striking example of the word “bark,” which has entirely different meanings based on context. These demonstrations helped demystify how AI models contextualize medical language.\nAI’s Clinical Impact: Real-World Examples in Nephrology\nWhat set Dr. Chan’s presentation apart was her focus on actionable, real-world applications of AI:\n\nSymptom Detection in Dialysis: Her team showed that AI-driven analysis of clinical notes outperformed patient surveys in identifying symptoms, with sensitivity scores of 0.92 versus 0.58-highlighting AI’s potential to uncover unreported concerns in hemodialysis patients. \n\n\n\nAmbient AI Scribes: Data from the Permanente Medical Group demonstrated tangible workflow improvements, including reduced after-hours EHR use, faster documentation, and high-quality note generation (48 out of 50 on scoring metrics).\nAI in Pathology: Dr. Chan showcased cutting-edge developments in nephropathology, where AI is used to segment kidney structures, detect interstitial fibrosis, and even predict eGFR decline based on histomorphological data.\n\nPractical Implementation: Prompt Engineering and Use Cases\nFor clinicians ready to experiment with AI tools, Dr. Chan introduced best practices in prompt engineering. Her guidance included being specific and clear, setting expectations for tone and output format, and iterating through revisions for optimal results. She encouraged participants to try generating LLM-powered diet recommendations customized for kidney patients, as a hands-on exercise in refining AI prompts.\nBalanced Perspective: Ethical and Practical Cautions\nWhile optimistic about AI’s future in kidney care, Dr. Chan was careful to underscore the limitations and ethical concerns:\n\nPrivacy: AI systems often run in the cloud-meaning uploading protected health information (PHI) poses serious risks.\nHallucinations: AI may fabricate content or citations, which necessitates careful cross-verification by clinicians.\nImplementation Barriers: Key challenges include the opaque (“black box”) nature of many models, the need for informed consent, and ensuring AI tools are accessible to all populations.\n\n\n😊 Thank for for your wonderful talk, we really enjoyed learning from you @Lili Chan ; my friends/colleagues @Jing Miao @Muhammad Yasir Baloch and I were truly fascinated by your presentation.\nWhat are the most significant obstacles preventing frontline nephrologists from using AI tools regularly? → Time, cost, trust, or lack of training? Happy to hear discussions from you all @Lili Chan @Karin Bergling @Len Usvyat @Kim Solez"
  },
  {
    "objectID": "background.html#nkf",
    "href": "background.html#nkf",
    "title": "Untitled",
    "section": "NKF",
    "text": "NKF\nAt the 2025 NKF Spring Clinical Meetings, Dr. @Navdeep Tangri presented an in-depth overview of how predictive—not generative—AI is transforming kidney care. As a global thought leader in nephrology innovation, he emphasized supervised machine learning’s (ML) role in accurate, efficient, and scalable clinical predictions.\n\nKey Insights and Applications\nSupervised ML dominates current AI use in nephrology, especially in predicting AKI, CKD progression, and dialysis-related complications.\n\nSimple models still work: Logistic regression often performs nearly as well as complex ML methods for structured datasets.\nImaging-driven AI is powerful: Deep learning excels in pathology and radiology, where it outperforms traditional biopsy interpretations and integrates with NLP to automate disease registries.\nCommercial models are maturing: Tools like Klinrisk and KidneyIntelX are externally validated and used across trials, payers, and health systems.\nIntegration is key: AI must be embedded into the clinical workflow, with transparency, physician leadership, and real-time decision support.\n\n\nThese are the performance metrics of machine learning models in kidney care, as presented in Dr. Navdeep Tangri’s NKF 2025 talk. It visualizes key AUC, sensitivity, and specificity values for acute kidney injury (AKI) prediction and intradialytic hypotension (IDH) prediction.\nML for CKD Progression Prediction\nObjective: Improve early identification and risk stratification of patients at risk of rapid CKD progression—without relying on invasive biomarkers or complex imaging.\n\n\nStrategic Implications for Practice\n\n\n\n\n\n\n\nImplementation Principle\nExample from Talk\n\n\nSimple ≠ Inferior\nKlinrisk uses basic labs but performs as well as complex tools\n\n\nClinical Workflow Integration First\nSONIC/Trajvis are designed for real-time flagging\n\n\nValidate &gt; Invent\nEvery model Dr. Tangri supports is externally validated\n\n\nPersonalization Matters\nModels stratify individual risk for resource targeting\n\n\n\nCritical Takeaways from Dr. Tangri\n\nDon’t always default to complex ML—many CKD progression problems are solvable with regression models.\nEvaluate models on three axes: Performance, Generalizability, and Clinical Integration.\nDemand external validation—not just ROC curves from a single health system.\nKFRE has limitations in early CKD; new models must cover CKD stages 1–5 and at-risk populations.\nTransparency and simplicity matter: Interpretability boosts clinician adoption.\n\nGiven the diversity of CKD patients (by race, SES, comorbidity), how do we ensure model generalizability across health systems and geographies? \nHow can we build a national registry of AI model performance across institutions to accelerate benchmarking, transparency, and trust? @Navdeep Tangri @Karandeep Singh @Lili Chan @Girish Nadkarni @Karin Bergling @Kim Solez"
  },
  {
    "objectID": "background.html#aki",
    "href": "background.html#aki",
    "title": "Untitled",
    "section": "AKI",
    "text": "AKI\nThe authors conducted a comprehensive search of PubMed, EMBASE, Web of Science, and Scopus up to August 2023. They screened 4816 articles and ultimately included 95 studies evaluating 302 externally validated ML models for AKI-related outcomes across 3.8 million admissions, covering both adult and pediatric populations.\nKey Findings\n\n🤔 So… Are These Models Ready for the Real World?\n🟢 Yes, because:\n\nThey show strong performance in predicting AKI across different settings.\nEven simple models work well (logistic regression = MVP).\n🔴 No, because:\nToo much variation between studies makes results hard to apply at the bedside.\nHigh bias in study design raises concerns.\nNot enough real-world testing in diverse patient groups.\n\n🧭 Where Do We Go From Here?\nTo bring AI into everyday kidney care, we need:\n\nStandardized predictors and outcomes 📏\nMore external validations 🏥\nBetter calibration and user-friendly tools ⚙️\nIntegration into clinical workflows with real-time feedback 👩‍⚕️👨‍⚕️\n\n💬 Food for Thought:\n\nWhat clinical features would you want an AKI prediction model to consider?\nShould hospitals invest in developing their own local ML models or adopt published ones?\nHow can we balance accuracy with explainability in high-risk predictions?\nWhat would it take for you to trust an AI model with real clinical decisions? @Kim Solez @Karin Bergling @Karandeep Singh @Jay Koyner @Kianoush Kashani"
  },
  {
    "objectID": "background.html#what-i-learned-at-era25-ai-in-nephrology--a-brief-synthesis-from-multiple-sessions-and-therere-more",
    "href": "background.html#what-i-learned-at-era25-ai-in-nephrology--a-brief-synthesis-from-multiple-sessions-and-therere-more",
    "title": "Untitled",
    "section": "📘 What I Learned at #ERA25: AI in Nephrology- A brief synthesis from multiple sessions and there’re more",
    "text": "📘 What I Learned at #ERA25: AI in Nephrology- A brief synthesis from multiple sessions and there’re more"
  },
  {
    "objectID": "background.html#reflections-on-the-role-of-ai-in-kidney-care",
    "href": "background.html#reflections-on-the-role-of-ai-in-kidney-care",
    "title": "Untitled",
    "section": "🤔 Reflections on the Role of AI in Kidney Care",
    "text": "🤔 Reflections on the Role of AI in Kidney Care\nAt ERA 2025 in Vienna, the conversation around artificial intelligence (AI) in nephrology has clearly evolved—from curiosity to implementation. This session, CME 9 – Artificial Intelligence for Nephrologists, showcased not just emerging tools but tangible outcomes from AI already being deployed in clinical practice. What stood out was the shift toward practical, integrated applications: systems that predict CKD progression using routine labs, guide anemia management in dialysis, and streamline EMR workflows with large language models. Yet, alongside innovation came important reminders about ethics, governance, and clinician accountability. Below is a structured summary of key insights, tools, and regulatory concerns discussed at the session. \n@Valentin Goutaudier @Len Usvyat"
  },
  {
    "objectID": "background.html#foundations-when-and-why-to-use-aiml",
    "href": "background.html#foundations-when-and-why-to-use-aiml",
    "title": "Untitled",
    "section": "🧠 Foundations: When and Why to Use AI/ML",
    "text": "🧠 Foundations: When and Why to Use AI/ML\n\nTalk: AI and Machine Learning Demystified\n\nWhen AI/ML Excels:\n\n\nComplex decision-making (e.g., multi-morbidity)\n\nLarge, multivariate data\n\nHeterogeneous syndromes like CKD\n\nPersonalized care—risk prediction, treatment targeting\n\n\n\n\nKey Benefits:\n\n\nCaptures nonlinear relationships\n\nIntegrates structured + unstructured data\n\nEnables scalable clinical decision support\n\n\n\n\nCore Limitations:\n\n\nSusceptible to biased/poor-quality data\n\nPoor generalizability without external validation\n\nOpaque “black box” nature → interpretability challenge"
  },
  {
    "objectID": "background.html#ai-tools-in-kidney-care-practice",
    "href": "background.html#ai-tools-in-kidney-care-practice",
    "title": "Untitled",
    "section": "🧪 AI Tools in Kidney Care Practice",
    "text": "🧪 AI Tools in Kidney Care Practice\n\n✅ CKD Progression Prediction with Klinrisk\n\n\nNo EHR or advanced biomarkers needed\n\nUses only routine labs\n\nOutperforms KDIGO and KFRE at 2–4 years\n\nAUC up to 0.86 at 4 years\n\nBuilt for population-level monitoring in T2DM patients\n\n\n\n\n🧬 Kidney Biopsy & Deep Learning\n\n\nAI outperforms manual review in morphologic quantification\n\nPredicts graft loss and segments glomerular lesions\n\n87% of regulatory-cleared AI tools = radiology/pathology\n\n\n\n\n🩸 AI-Powered Anemia Management\nTool: Anemia Control Model (ACM)\n\n\nActive since 2013 in &gt;100 clinics\n\nDecision support for ESA/iron dosing\n\nOutcomes:\n\n\n25% ESA use reduction\n\n12% lower hospitalization rate\n\n12-year safety track record\n\n\n\n\n\n\n💻 Large Language Models (LLMs) in EMRs\n\n\nGPT-based tools increasingly embedded in EMRs\n\nUse cases:\n\n\nInbox triage\n\nProgress note generation\n\nPrior auths & billing support\n\nPatient-specific reminders between visits"
  },
  {
    "objectID": "background.html#regulatory-ethical-considerations",
    "href": "background.html#regulatory-ethical-considerations",
    "title": "Untitled",
    "section": "⚖️ Regulatory & Ethical Considerations",
    "text": "⚖️ Regulatory & Ethical Considerations\nSpeaker: Dr. Pierpaolo Maio\nTalk: AI in Nephrology – Useful or Dangerous? (CON)\n\nRegulatory Frameworks (EU AI Act, MDR, PLD):\n\n\nAI for nephrology = “high-risk” designation\n\nRequires CE mark, ISO 13485, surveillance\n\nDevelopers + clinicians share legal liability\n\n\n\n\nGovernance Imperatives:\n\n\nMandatory human oversight\n\nTransparent AI logic\n\nHigh-value/low-risk cases prioritized\n\nIncident reporting for AI-related errors mandatory"
  },
  {
    "objectID": "background.html#final-thoughts",
    "href": "background.html#final-thoughts",
    "title": "Untitled",
    "section": "✨ Final Thoughts",
    "text": "✨ Final Thoughts\nAI is transitioning from conceptual hype to practical utility in nephrology. As seen at #ERA25, its applications—from CKD prediction and glomerular image analysis to automated ESA dosing—are yielding real-world benefits. However, success hinges on data quality, regulatory compliance, and ongoing clinician oversight.\nThe future of AI in nephrology is not just about algorithms—it is about responsible integration, trust-building, and ensuring that every decision still begins and ends with the patient.\nThis is just my personal summary of what I learned from the #ERA25 session on AI in nephrology. I may have missed some points, and others might have different takeaways. Feel free to add or share more insights! 😊🙏"
  },
  {
    "objectID": "background.html#how-physicians-can-prepare-for-generative-ai.-jama-internal-medicine-oct-13-2025",
    "href": "background.html#how-physicians-can-prepare-for-generative-ai.-jama-internal-medicine-oct-13-2025",
    "title": "Untitled",
    "section": "“How Physicians Can Prepare for Generative AI.” JAMA Internal Medicine, Oct 13, 2025",
    "text": "“How Physicians Can Prepare for Generative AI.” JAMA Internal Medicine, Oct 13, 2025\n\nGenerative artificial intelligence (GenAI) is no longer a futuristic concept-it has arrived in clinics, hospitals, and patient portals. While today’s AI scribes and documentation tools may seem revolutionary, the authors emphasize that these are just the beginning. The next generation of GenAI will not only summarize notes-it will help diagnose, counsel, and even manage clinical conditions. Physicians who understand how to integrate and supervise these systems will lead the transformation of healthcare.\n\n\n\n\n\nUsing AI as a Partner\nTreat AI as a partner, not just a tool. It can reason and communicate but requires clinical oversight and validation.\nPrompting matters. The way a question is framed affects the quality of the response; rephrasing and specifying the AI’s “role” (e.g., “You are an emergency physician…”) can improve accuracy.\nData input quality determines output quality. Clinicians should provide complete, relevant data (e.g., key labs, discharge summaries).\nInteractive learning is key. Physicians should probe chatbot reasoning-ask why and where it obtained information to uncover reasoning flaws.\nClinical Workflow and Responsibility\nGenAI will reshape workflows. Clinicians will shift from writing to editing, as AI scribes generate notes and summaries.\nLegal accountability remains with clinicians. Physicians are responsible for AI-generated documentation until regulatory oversight evolves.\nAI outputs require review. Clinicians must verify accuracy before signing notes or using AI-suggested recommendations.\nFuture FDA oversight will likely extend to documentation and decision-support tools, but clinicians should demand performance validation now.\nEvolving Physician–Patient Relationship\nPatient expectations are changing. Many patients already use GenAI to interpret symptoms, lab results, and possible diagnoses.\nThe physician’s role will shift-from sole knowledge authority to expert adjudicator who validates AI- or patient-generated insights.\nHuman skills gain importance. Empathy, physical examination, and shared decision-making will distinguish clinicians from algorithms.\nEducation and Skill Development\nAll clinicians need foundational AI literacy. Understanding how LLMs work is now part of medical competence.\nFormal training is emerging. Institutions like NYU Langone and Harvard offer GenAI prompt training; the American College of Physicians offers short AI courses.\nHands-on experience with safe, HIPAA-compliant tools (e.g., Microsoft Copilot, ChatGPT Enterprise) helps clinicians build confidence.\nEthical literacy is essential. Understanding bias, hallucination, and patient data protection is as important as technical skills.\nFuture Outlook\nGenAI will evolve from assistant to agent. Future “agentic” systems may independently collect histories, counsel patients, or generate management plans-requiring new oversight, ethics frameworks, and trials to assess safety and efficacy.\nhttps://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2840192"
  },
  {
    "objectID": "index.html#slides",
    "href": "index.html#slides",
    "title": "Data and Generative AI",
    "section": "Slides",
    "text": "Slides\nView slides in full screen\n\n\nMarch 17, 2026\n\n\n\nLuca Neri, MD, PhD  Renal Research Institute, New York, NY\n\n\n\n\n\nKurtis A. Pivert, MS, CAPX  American Society of Nephrology, Washington, DC\n\n\n\n\nAgenda\n\n\n\n9:00 a.m.\nIntroduction\n\n\n9:10 a.m.\nHands-On Exercise 1\n\n\n9:20 a.m.\nHands-On Exercise 2\n\n\n9:30 a.m.\nHands-On Exercise 3\n\n\n9:40 a.m.\nWrap-Up"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Prompt Best Practices\n\nThe fundamentals that work across all LLMs:\n\nRole/Persona: Who the AI should act as\nContext: What background info it needs\nTask: What you want it to do explicitly\nFormat: How you want the output structured\nConstraints: What to avoid or limits to follow\n\n\n Source: Reddit: Best Practices for AI Prompting 2025?\n\n\nCO-STAR Prompt Framework\n\nA great prompt usually includes these specific elements to provide context and direction:\n\nContext: Provide background information on the task.\nObjective: Define exactly what you want me to do.\nStyle: Specify a writing style (e.g., professional, humorous, poetic).\nTone: Set the emotional vibe (e.g., empathetic, direct, enthusiastic).\nAudience: Who is this for? (e.g., a CEO, a 5-year-old, a software engineer).\nResponse: Define the format (e.g., a table, a list, a JSON object).\n\nAdditional Components\n\nFew-Shot Prompting/Using Examples: Provide examples of desired output and formatting (e.g., programming language, file type [CSV, Markdown, PNG])\nUse Delimiters: Use triple quotes (“““), XML tags (), or dashes (—) to wrap your content (e.g., Summarize the text delimited by triple quotes:”““[Insert article]”““)\n\n\n\nSource: Google Gemini (Prompt: do you have any prompt engineering best practices)\n\n\nChicago Booth Algorithmic Bias Initiative\n\nAlgorithmic Bias Initiative Website\nAlgorithmic Bias Playbook\n\n\n\nMelissa Van Bussel posit::conf 2024 Keynote\n\nPractical Tips for Using Generative AI in Data Science Workflows\nSlides\n\n\n\nAdditional Prompt Engineering Resources\n\nAnthropic\n\nPrompt Engineering Guide Overview\nPrompt Engineering Interactive Tutorial\n\n\n\nOpenAI\n\nOpenAI Developers Cookbook\nOpenAI ChatGPT Cookbook\nOpenAI Cookbook Code Repo\nBest Practices for Prompt Engineering with the OpenAI API\nPrompt Engineering\n\n\n\nGoogle\n\nPrompt engineering: overview and guide\n\n\n\n10 Best Practices for Prompt Engineering With Any Model\n\nPromptHub\n\n\n\nGitHub: Awesome AI System Prompts\n\nAwesome AI System Prompts"
  }
]